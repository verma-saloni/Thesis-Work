{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwnhzdU4UFL9wusfrJGQGT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/verma-saloni/Thesis-Work/blob/main/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode\n",
        "!pip install word2number\n",
        "!pip install contractions\n",
        "!python -m spacy download en_core_web_md "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhWwI2D2P0lE",
        "outputId": "7bcba7bb-4a33-48d4-e034-0d008c73ca9b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n",
            "Requirement already satisfied: word2number in /usr/local/lib/python3.7/dist-packages (1.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.58)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.10.8)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jszg-B0Rfxu",
        "outputId": "ec333c96-f113-4524-8da3-e09feb0cd5b4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n53Cmg0Dpfu",
        "outputId": "6be69f79-b0d6-4181-dfb7-c16be3bfb9e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "string.punctuation\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import unidecode\n",
        "from word2number import w2n\n",
        "import contractions\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "# load spacy model, can be \"en_core_web_sm\" as well\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive._mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmBUBYm1dDvu",
        "outputId": "60804a9b-e45e-4612-d46f-47634d46d4dd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_fake = pd.read_csv('/content/drive/MyDrive/gossipcop_fake.csv')\n",
        "df_real = pd.read_csv('/content/drive/MyDrive/gossipcop_real.csv')\n",
        "df_fake['labelML']=0\n",
        "df_real['labelML']=1\n",
        "df= df_fake.head(50).append(df_real.head(50))\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "3-MHacetDtNT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweet_count'] = df['tweet_ids'].str.split('\\t').str.len()"
      ],
      "metadata": {
        "id": "lXMntRj7D8S6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install spacy\n",
        "import spacy\n",
        "spacy.load('en')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEGeqKh6hnyd",
        "outputId": "46a0177e-f9c2-4135-f63e-42af52ea6455"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7fbab98ab6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "# exclude words from spacy stopwords list\n",
        "deselect_stop_words = ['no', 'not']\n",
        "for w in deselect_stop_words:\n",
        "    nlp.vocab[w].is_stop = False\n",
        "\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    \"\"\"remove html tags from text\"\"\"\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text(separator=\" \")\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "def remove_whitespace(text):\n",
        "    \"\"\"remove extra whitespaces from text\"\"\"\n",
        "    text = text.strip()\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    \"\"\"remove accented characters from text, e.g. café\"\"\"\n",
        "    text = unidecode.unidecode(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def expand_contractions(text):\n",
        "    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n",
        "    text = contractions.fix(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def text_preprocessing(text, accented_chars=True, contractions=True, \n",
        "                       convert_num=True, extra_whitespace=True, \n",
        "                       lemmatization=True, lowercase=True, punctuations=True,\n",
        "                       remove_html=True, remove_num=True, special_chars=True, \n",
        "                       stop_words=True):\n",
        "    \"\"\"preprocess text with default option set to true for all steps\"\"\"\n",
        "    if remove_html == True: #remove html tags\n",
        "        text = strip_html_tags(text)\n",
        "    if extra_whitespace == True: #remove extra whitespaces\n",
        "        text = remove_whitespace(text)\n",
        "    if accented_chars == True: #remove accented characters\n",
        "        text = remove_accented_chars(text)\n",
        "    if contractions == True: #expand contractions\n",
        "        text = expand_contractions(text)\n",
        "    if lowercase == True: #convert all characters to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "    doc = nlp(text) #tokenise text\n",
        "\n",
        "    clean_text = []\n",
        "    \n",
        "    for token in doc:\n",
        "        flag = True\n",
        "        edit = token.text\n",
        "        # remove stop words\n",
        "        if stop_words == True and token.is_stop and token.pos_ != 'NUM': \n",
        "            flag = False\n",
        "        # remove punctuations\n",
        "        if punctuations == True and token.pos_ == 'PUNCT' and flag == True: \n",
        "            flag = False\n",
        "        # remove special characters\n",
        "        if special_chars == True and token.pos_ == 'SYM' and flag == True: \n",
        "            flag = False\n",
        "        # remove numbers\n",
        "        if remove_num == True and (token.pos_ == 'NUM' or token.text.isnumeric()) \\\n",
        "        and flag == True:\n",
        "            flag = False\n",
        "        # convert number words to numeric numbers\n",
        "        if convert_num == True and token.pos_ == 'NUM' and flag == True:\n",
        "            edit = w2n.word_to_num(token.text)\n",
        "        # convert tokens to base form\n",
        "        elif lemmatization == True and token.lemma_ != \"-PRON-\" and flag == True:\n",
        "            edit = token.lemma_\n",
        "        # append tokens edited and not removed to list \n",
        "        if edit != \"\" and flag == True:\n",
        "            clean_text.append(edit)        \n",
        "    return TreebankWordDetokenizer().detokenize(clean_text)"
      ],
      "metadata": {
        "id": "nFxvjXpBPSA-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['title_preprocessed'] = df['title'].apply(text_preprocessing)"
      ],
      "metadata": {
        "id": "9c_iyRgdS3KI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s  = df['title_preprocessed'].str.split(' ').explode().to_frame()\n",
        "\n",
        "cols = s['title_preprocessed'].drop_duplicates(keep='first').tolist()\n",
        "\n",
        "df2 = pd.concat([df, pd.crosstab(s.index, s[\"title_preprocessed\"])[cols]], axis=1).replace(\n",
        "    {1: 1, 0: 0}\n",
        ")\n",
        "print(df2.columns)\n",
        "\n",
        "df2= df2.drop(['id','news_url','title','tweet_ids','title_preprocessed'],axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlhhcCHuTHPa",
        "outputId": "c69df321-41e4-46e5-cc6f-860c176ca8b5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'news_url', 'title', 'tweet_ids', 'labelML', 'tweet_count',\n",
            "       'title_preprocessed', 'miley', 'cyrus', 'liam',\n",
            "       ...\n",
            "       'iggy', 'azalea', 'f', 'moron', 'quavo', 'delilah', 'son', 'suicide',\n",
            "       'heartbreaking', 'goodbye'],\n",
            "      dtype='object', length=543)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.reset_index(drop=True, inplace=True)\n",
        "X, y = df2.iloc[:, 1:], df2['labelML']\n",
        "\n",
        "# Create the training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)\n",
        "\n",
        "# Instantiate the XGBClassifier: xg_cl\n",
        "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
        "\n",
        "# Fit the classifier to the training set\n",
        "xg_cl.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set: preds\n",
        "preds = xg_cl.predict(X_test)\n",
        "\n",
        "# Compute the accuracy: accuracy\n",
        "accuracy = float(np.sum(preds == y_test)) / y_test.shape[0]\n",
        "print(\"accuracy: %f\" % (accuracy))"
      ],
      "metadata": {
        "id": "dkOigzN4drJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f97a8487-5d98-4b21-f682-a5cd640e6c6d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.800000\n"
          ]
        }
      ]
    }
  ]
}