{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of politifact_graph.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/verma-saloni/Thesis-Work/blob/main/07_25_rq3_politifact_graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xpTeMbYZwmf7"
      },
      "outputs": [],
      "source": [
        "!pip -qq install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "from pathlib import Path\n",
        "base_dir = Path(\"/gdrive/MyDrive/ResearchFND\")\n",
        "assert base_dir.exists()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxNEaltli8H_",
        "outputId": "2ef65b7f-2b2e-42e8-aad8-8718e9d4f848"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Twt user graphs"
      ],
      "metadata": {
        "id": "60XHa2QqsF2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import jsonlines"
      ],
      "metadata": {
        "id": "7lCdLfhNsEyM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = nx.DiGraph()"
      ],
      "metadata": {
        "id": "Q0vjdfpgsQJ_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open(base_dir/\"followers.jsonl\") as reader:\n",
        "    for line in reader:\n",
        "        v = line[\"user_id\"]\n",
        "        for u in line[\"followers\"]:\n",
        "            graph.add_edge(u, v)"
      ],
      "metadata": {
        "id": "KnrRSt2Qsb6O"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open(base_dir/\"following.jsonl\") as reader:\n",
        "    for line in reader:\n",
        "        u = line[\"user_id\"]\n",
        "        for v in line[\"following\"]:\n",
        "            graph.add_edge(u, v)"
      ],
      "metadata": {
        "id": "Y7DLQ78as4pZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph.number_of_nodes(), graph.number_of_edges()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCN3DvNQtiJR",
        "outputId": "b4f0f440-36e5-4418-f2c4-c06728e8c8aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31792, 48408)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "followed = [n for n in graph.nodes if graph.in_degree(n)>2]\n",
        "len(followed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-x18_ZZulOd",
        "outputId": "3e4fd2ed-76e7-4080-8b40-912de5acedea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "444"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "sample_nodes = random.sample(followed, 12)\n",
        "sg = graph.edge_subgraph(graph.in_edges(sample_nodes))\n",
        "sg.number_of_nodes(), sg.number_of_edges()"
      ],
      "metadata": {
        "id": "ytEH363Cu7e_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc1b4d5-1f1f-479c-ff5a-21ea832bbfed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23, 44)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nx.draw(sg, node_size=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "yYc34e6ts7PN",
        "outputId": "0cbcf5a1-1ffa-4dd0-cd68-8969587f2f12"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Node embeddings"
      ],
      "metadata": {
        "id": "qbRXmgXCwoNj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28effea1-9db8-4c0f-b037-3df8dadbea17",
        "outputId": "18bb945e-871c-4dea-dd0a-ccdbb85fe832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for csrgraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nodevectors (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -Uqq csrgraph nodevectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csrgraph as cg\n",
        "from nodevectors import Node2Vec"
      ],
      "metadata": {
        "id": "qvL85vFzr2XK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### user vectors"
      ],
      "metadata": {
        "id": "odOQGulI-EeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n2v = Node2Vec(n_components=128, walklen=10, epochs=10)"
      ],
      "metadata": {
        "id": "RvlSsu6jvXS7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n2v.fit(graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHoiGFydwFMk",
        "outputId": "af696748-7047-4823-c293-acffb1c3f7ff"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making walks... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done, T=5.25\n",
            "Mapping Walk Names... Done, T=7.71\n",
            "Training W2V... WARNING: gensim word2vec version is unoptimizedTry version 3.6 if on windows, versions 3.7 and 3.8 have had issues\n",
            "Done, T=84.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n2v.predict(followed[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RicJpnXuwJhE",
        "outputId": "df782e17-8f72-4e30-9eb1-ea08fb0437f1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.3675863 ,  0.1282412 ,  0.22299092, -0.31979853,  0.18588693,\n",
              "       -0.11166151, -0.25063002, -0.00243194,  0.3993893 ,  0.2265691 ,\n",
              "        0.24865997, -0.00837706, -0.07028485, -0.20618919,  0.23557512,\n",
              "       -0.14061683,  0.30435127, -0.39891508,  0.06668442, -0.33265588,\n",
              "       -0.02163144, -0.36777756, -0.16099092,  0.21908268, -0.19462305,\n",
              "       -0.40506038, -0.25495267, -0.45383984, -0.5653268 ,  0.35322955,\n",
              "       -0.5327221 ,  0.52026945,  0.32708022,  0.0113182 ,  0.40652302,\n",
              "        0.21851438,  0.10024845,  0.35892075,  0.40906376,  0.04410933,\n",
              "       -0.50577044,  0.21270105,  0.5493385 ,  0.15493374,  0.09996232,\n",
              "       -0.37725475, -0.37640077,  0.2239793 ,  0.17491885, -0.2525284 ,\n",
              "        0.16396652,  0.25392124,  0.3824317 , -0.03385516,  0.4343061 ,\n",
              "       -0.01122301,  0.03740161,  0.06381975, -0.02905279, -0.05415915,\n",
              "       -0.09567568, -0.35780594,  0.2964456 , -0.5187444 ,  0.10663714,\n",
              "       -0.30514914,  0.13067374, -0.05766976, -0.25125933,  0.02838036,\n",
              "       -0.10923844,  0.3680409 ,  0.1466139 , -0.29836094,  0.4735993 ,\n",
              "        0.41329947, -0.26242357, -0.5369673 , -0.43316695, -0.168669  ,\n",
              "        0.18525039, -0.14915812,  0.4306212 , -0.1937244 ,  0.535601  ,\n",
              "       -0.06732793, -0.42905512, -0.11690461,  0.29821655, -0.44861498,\n",
              "        0.20499462, -0.34825817,  0.07389084, -0.3002827 , -0.31174114,\n",
              "        0.05700825,  0.56036174,  0.2569448 ,  0.2916066 , -0.18752065,\n",
              "       -0.42436475,  0.2378338 , -0.4374494 , -0.32618782, -0.21294492,\n",
              "        0.3716916 , -0.12359514,  0.25273126, -0.5215603 ,  0.18840842,\n",
              "       -0.20114021, -0.3934462 ,  0.49193785,  0.04989393,  0.12659249,\n",
              "        0.12628333, -0.33194947, -0.38520122, -0.4027925 , -0.27302036,\n",
              "        0.11351606,  0.12338854, -0.22894095, -0.24686773,  0.19909653,\n",
              "        0.02797645,  0.00258626, -0.26178077], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## user-article graph"
      ],
      "metadata": {
        "id": "NHdG78wwVumY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast"
      ],
      "metadata": {
        "id": "7tPiW_FBWO8s"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(base_dir/'politifact_agg.csv', index_col=0)\n",
        "df.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "0BsiweMKV3Kt",
        "outputId": "c2081efb-8798-419c-fdb1-3753c3ad4b1e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title text tweets  \\\n",
              "0   Actress Emma Stone ‘For the first time in his...  NaN     []   \n",
              "1   Breaking President Trump makes English the of...  NaN     []   \n",
              "\n",
              "                                            retweets label  url tweet_ids  \\\n",
              "0  ['1020554564334964741', '1020817527046197248',...  fake  NaN        []   \n",
              "1                                                 []  fake  NaN        []   \n",
              "\n",
              "   num_retweets  log_num_retweets  num_tweets  log_num_tweets  \n",
              "0          2911          7.976595           0             0.0  \n",
              "1             0          0.000000           0             0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c002453-36bc-45de-be89-5925a63ed715\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>tweets</th>\n",
              "      <th>retweets</th>\n",
              "      <th>label</th>\n",
              "      <th>url</th>\n",
              "      <th>tweet_ids</th>\n",
              "      <th>num_retweets</th>\n",
              "      <th>log_num_retweets</th>\n",
              "      <th>num_tweets</th>\n",
              "      <th>log_num_tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Actress Emma Stone ‘For the first time in his...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>['1020554564334964741', '1020817527046197248',...</td>\n",
              "      <td>fake</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>2911</td>\n",
              "      <td>7.976595</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Breaking President Trump makes English the of...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>fake</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c002453-36bc-45de-be89-5925a63ed715')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7c002453-36bc-45de-be89-5925a63ed715 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7c002453-36bc-45de-be89-5925a63ed715');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweets'] = df.tweets.map(ast.literal_eval)"
      ],
      "metadata": {
        "id": "cruZFCpNWRLb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df[df.tweets.map(len) > 0]"
      ],
      "metadata": {
        "id": "FmEDMzaPWrT2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_tweeted = df.tweets.map(lambda x: [int(e['user_id']) for e in x])"
      ],
      "metadata": {
        "id": "wH5i98HOWz1w"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(users_tweeted), sum(users_tweeted.map(len) > 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exnDy-5ud_uJ",
        "outputId": "8769dfed-4e4a-46ce-aa33-1bba9afcb1e9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(894, 149)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users_tweeted[users_tweeted.map(len) > 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RfPGb15XgKE",
        "outputId": "2b74b633-6dc4-405d-e2c0-87fa24d6b183"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8      [1191949489, 51600575, 2984014208, 15496777, 4...\n",
              "10     [24636033, 22473015, 129262130, 181289232, 181...\n",
              "11     [490975378, 116344653, 79423345, 53347857, 842...\n",
              "12     [586165566, 393939273, 891366735064039425, 991...\n",
              "14     [886818711105138688, 172947702, 993606350, 428...\n",
              "                             ...                        \n",
              "875    [14378429, 3187248602, 484966779, 950864702669...\n",
              "880    [2236833270, 22250210, 54835665, 347507710, 45...\n",
              "884    [2582795785, 736847516537368576, 7769521655752...\n",
              "887    [348175292, 810185480469880833, 84976273735388...\n",
              "893    [747513995096055808, 19086760, 27623387, 12455...\n",
              "Name: tweets, Length: 149, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_users = set()\n",
        "for e in users_tweeted:\n",
        "    unique_users.update(set(e))\n",
        "\n",
        "len(unique_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUjaXHhFYIMc",
        "outputId": "e52aaed2-9f68-4fef-acab-53ada5c35d4a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40723"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min(unique_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHgKrYoEYTZQ",
        "outputId": "fabb6bc1-6e47-4785-d6dd-922ecb441221"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1688"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for u in unique_users:\n",
        "    if u in graph:\n",
        "        i += 1\n",
        "print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF3sOiF_bcip",
        "outputId": "a7453b0b-b044-4a12-d27b-0afe53706dd6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users_retweeted = df.retweets.map(lambda x: [int(e) for e in ast.literal_eval(x)])"
      ],
      "metadata": {
        "id": "NPSabn0Je819"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in users_retweeted:\n",
        "    unique_users.update(set(e))\n",
        "\n",
        "len(unique_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UevgMKgIfeqt",
        "outputId": "400fcb74-1851-4474-a42c-f8206b507df2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "595543"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for u in unique_users:\n",
        "    if u in graph:\n",
        "        i += 1\n",
        "print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQttmXbOfwz3",
        "outputId": "d5d32941-f89a-418e-fad1-984745863f5d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, l in users_tweeted.iteritems():\n",
        "    if not len(l):\n",
        "        continue\n",
        "    a_node = f\"a{i}\"\n",
        "    graph.add_edges_from([(a_node, u_node) for u_node in l if u_node in graph])\n",
        "\n",
        "graph.number_of_nodes(), graph.number_of_edges()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TacXl683Z-BC",
        "outputId": "9c6ffc5f-216e-49c2-e330-f35668773c67"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31896, 49576)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = users_tweeted.map(len)\n",
        "tmp.min(), tmp.mean(), tmp.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lnh-YSHaYlH",
        "outputId": "ee149d97-ddb4-44c0-ab43-60a1c645b8e6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 70.42953020134229, 21984)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GNN"
      ],
      "metadata": {
        "id": "gI6AtSffmcqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data"
      ],
      "metadata": {
        "id": "FfaOg08xmfEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "xjD3rF-MmwKa",
        "outputId": "5228d3b5-e600-423c-cb01-9fa4cd903d9b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dgl\n",
            "  Downloading dgl-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.6)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl) (4.64.0)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[K     |████████████████████████████████| 281 kB 69.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.7.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Installing collected packages: psutil, dgl\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed dgl-0.9.0 psutil-5.9.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp52MLR1mdxA",
        "outputId": "cc330e1b-98b8-4e70-870d-70300830c133"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u2i = {}\n",
        "\n",
        "follow_src = []\n",
        "follow_dst = []\n",
        "with jsonlines.open(base_dir/\"followers.jsonl\") as reader:\n",
        "    for line in reader:\n",
        "        v = line[\"user_id\"]\n",
        "        if v not in u2i:\n",
        "            u2i[v] = len(u2i)\n",
        "        for u in line[\"followers\"]:\n",
        "            if u not in u2i:\n",
        "                u2i[u] = len(u2i)\n",
        "            follow_src.append(u2i[u])\n",
        "            follow_dst.append(u2i[v])"
      ],
      "metadata": {
        "id": "h3XOCaEwoMvx"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open(base_dir/\"following.jsonl\") as reader:\n",
        "    for line in reader:\n",
        "        u = line[\"user_id\"]\n",
        "        if u not in u2i:\n",
        "            u2i[u] = len(u2i)\n",
        "        for v in line[\"following\"]:\n",
        "            if v not in u2i:\n",
        "                u2i[v] = len(u2i)\n",
        "            follow_src.append(u2i[u])\n",
        "            follow_dst.append(u2i[v])"
      ],
      "metadata": {
        "id": "sWJFoJ_4oMvz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_src = []\n",
        "tweet_dst = []\n",
        "\n",
        "for v, l in users_tweeted.iteritems():\n",
        "    if not len(l):\n",
        "        continue\n",
        "    for u in l:\n",
        "        if u in u2i:\n",
        "            tweet_src.append(u2i[u])\n",
        "            tweet_dst.append(v)"
      ],
      "metadata": {
        "id": "DWM9MEXDpiyX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embs = np.load(base_dir/'sbert_fulltext_embeddings.npy')\n",
        "text_embs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxkWUv799WnY",
        "outputId": "83209797-5819-46c3-c541-f0a59b71fe35"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(894, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_embs= pd.read_csv('updatedEmbs.csv')\n",
        "final_embs= final_embs.drop('Unnamed: 0',axis=1)\n",
        "final_embs = final_embs.reset_index(drop=True)\n",
        "print(final_embs)\n",
        "\n",
        "text_embs= final_embs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT3XHzLyHf_k",
        "outputId": "becced59-792b-4cf7-c40d-5cd29261f39f"
      },
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                0             1           0.1           1.1        0.1.1  \\\n",
            "0        1.349461     -2.602859      2.833252     -2.743249     6.893370   \n",
            "1        3.731273     -4.053496      2.255937     -2.753675     7.221067   \n",
            "2        2.332864     -2.908929      2.941076     -3.133755     7.399475   \n",
            "3        1.924908     -2.274820      1.640638     -2.479522     5.149405   \n",
            "4       -3.909992      4.973126     -4.583091      7.322591   -11.548117   \n",
            "..            ...           ...           ...           ...          ...   \n",
            "889      2.716950     -3.773576      3.432483     -3.184185     6.446299   \n",
            "890      2.423111     -2.884316      2.270691     -3.015852     6.600163   \n",
            "891     -2.720943      3.260452     -4.072395      3.835468    -7.015322   \n",
            "892      2.341196     -3.274098      3.072714     -3.649316     5.207879   \n",
            "893 -49879.184000  33768.227000 -14806.883000  11694.690000 -2608.811000   \n",
            "\n",
            "           1.1.1     0.1.1.1      1.1.1.1    0.1.1.1.1    1.1.1.1.1  ...  \\\n",
            "0      -3.390698    1.659329    -3.012199     3.436132    -2.051579  ...   \n",
            "1      -4.423811    1.772402    -4.427463     3.389840    -3.910992  ...   \n",
            "2      -3.701832    1.529097    -3.323052     2.685679    -2.423942  ...   \n",
            "3      -2.522085    2.035665    -4.024216     3.072954    -1.651766  ...   \n",
            "4       5.724316   -2.577478     6.573018    -5.460110     2.931455  ...   \n",
            "..           ...         ...          ...          ...          ...  ...   \n",
            "889    -5.202756    3.625214    -3.966662     3.847220    -3.245514  ...   \n",
            "890    -4.685534    2.986126    -4.408060     3.591517    -2.934106  ...   \n",
            "891     3.222818   -2.212169     4.496822    -3.261340     2.588654  ...   \n",
            "892    -2.868720    2.375783    -3.565799     2.690030    -2.718827  ...   \n",
            "893  2288.010500 -284.562840  1012.947500 -2789.188000  2361.170200  ...   \n",
            "\n",
            "          758       759       760       761       762       763       764  \\\n",
            "0    0.012692  0.059102  0.000541  0.006523 -0.018735  0.046029 -0.031323   \n",
            "1   -0.017646  0.061697  0.007149  0.000070  0.010949  0.033709 -0.033934   \n",
            "2    0.039987  0.010057 -0.038091 -0.033632 -0.018176 -0.017782  0.028502   \n",
            "3   -0.070302 -0.045334  0.016006  0.018444  0.056975  0.053900  0.043913   \n",
            "4    0.007845  0.069599 -0.017022  0.010712 -0.014984 -0.001413 -0.022211   \n",
            "..        ...       ...       ...       ...       ...       ...       ...   \n",
            "889 -0.026921  0.033994  0.025864 -0.034472  0.024374 -0.022076 -0.001890   \n",
            "890  0.092023 -0.017697  0.014944 -0.065511  0.016395  0.007684 -0.016207   \n",
            "891 -0.050173  0.037019 -0.048023  0.000220 -0.033158  0.075531  0.064022   \n",
            "892 -0.008388  0.008983 -0.058522 -0.008533 -0.016264  0.003724 -0.005826   \n",
            "893  0.025953  0.049041 -0.015561 -0.064802  0.025398  0.007854 -0.010056   \n",
            "\n",
            "          765       766       767  \n",
            "0    0.021993 -0.059348  0.010890  \n",
            "1   -0.006214 -0.046594 -0.020140  \n",
            "2    0.009641 -0.035604 -0.009934  \n",
            "3    0.003843 -0.038083  0.000696  \n",
            "4   -0.025620  0.007522 -0.037539  \n",
            "..        ...       ...       ...  \n",
            "889  0.006269 -0.019845  0.016206  \n",
            "890 -0.008501  0.049251 -0.022232  \n",
            "891 -0.000590 -0.031904 -0.002240  \n",
            "892 -0.020846 -0.027097 -0.008377  \n",
            "893  0.026847 -0.055978 -0.010197  \n",
            "\n",
            "[894 rows x 786 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "follow_src = torch.tensor(follow_src)\n",
        "follow_dst = torch.tensor(follow_dst)\n",
        "tweet_src = torch.tensor(tweet_src)\n",
        "tweet_dst = torch.tensor(tweet_dst)\n",
        "\n",
        "hetero_graph = dgl.heterograph({\n",
        "    ('user', 'follow', 'user'): (follow_src, follow_dst),\n",
        "    ('user', 'followed-by', 'user'): (follow_dst, follow_src),\n",
        "    ('user', 'tweet', 'article'): (tweet_src, tweet_dst),\n",
        "    ('article', 'tweeted-by', 'user'): (tweet_dst, tweet_src)})\n",
        "\n",
        "hetero_graph.nodes['user'].data['feat'] = torch.arange(hetero_graph.num_nodes('user'))  \n",
        "hetero_graph.nodes['article'].data['feat'] = torch.tensor(text_embs.values.tolist())\n",
        "hetero_graph.nodes['article'].data['label'] = torch.tensor((df.label==\"real\").to_numpy()).long()"
      ],
      "metadata": {
        "id": "ui3y2a80muWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce6fa3e-7317-4d63-d00e-5e565ce12cdd"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hetero_graph.number_of_nodes(ntype='article'), hetero_graph.number_of_nodes(ntype='user')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcmqOPpJsK6Z",
        "outputId": "fb038a21-718d-44f3-8b2d-88fe3c73e897"
      },
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(894, 31792)"
            ]
          },
          "metadata": {},
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hetero_graph.nodes['article'].data['train_mask'] = torch.zeros(hetero_graph.number_of_nodes(ntype='article'), dtype=torch.bool).bernoulli(0.8)"
      ],
      "metadata": {
        "id": "2LboKsvbsJAR"
      },
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "a_NnqbhIvgQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_dict(d):\n",
        "    for k, v in d.items():\n",
        "        d[k] = v.flatten(1)\n",
        "    return d"
      ],
      "metadata": {
        "id": "LqjDfQQYCzml"
      },
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def _d_emb(): return 64\n",
        "\n",
        "d_emb_dict = defaultdict(_d_emb)"
      ],
      "metadata": {
        "id": "oSGL7euSjzR-"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NodeEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, n_nodes, d_in, d_emb, proj_nodes=None, embed_nodes=None):\n",
        "        super().__init__()\n",
        "        self.proj_nodes = proj_nodes if proj_nodes is not None else list(d_in.keys())\n",
        "        self.embed_nodes = embed_nodes if embed_nodes is not None else list(n_nodes.keys())\n",
        "        self.emb = nn.ModuleDict({k:nn.Embedding(n_nodes[k], d_emb) for k in self.embed_nodes})\n",
        "        self.proj = nn.ModuleDict({k:nn.Linear(d_in[k], d_emb, bias=False) for k in self.proj_nodes})\n",
        "        self.init()\n",
        "\n",
        "    def forward(self, nx):\n",
        "        out = {}\n",
        "        for k, m  in self.emb.items():\n",
        "            out[k] = m(nx[k])\n",
        "        for k, m  in self.proj.items():\n",
        "            out[k] = m(nx[k])\n",
        "        return out\n",
        "\n",
        "    def init(self):\n",
        "        for _, m in self.emb.items():\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "        for _, m in self.proj.items():\n",
        "            torch.nn.init.xavier_uniform_(m.weight)"
      ],
      "metadata": {
        "id": "YGqNa5UVgfio"
      },
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb = NodeEmbedding({k:hetero_graph.num_nodes(k) for k in [\"user\"]}, {\"article\":text_embs.shape[1]}, 64)"
      ],
      "metadata": {
        "id": "HZMUKp0Xl-8v"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    x = emb(hetero_graph.ndata['feat'])"
      ],
      "metadata": {
        "id": "BTV2S60Hwcut"
      },
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in x.items():\n",
        "    print(k, v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPxiP5oiLFPb",
        "outputId": "5f0a5f1f-6516-49dc-994e-8adbf498ffea"
      },
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user torch.Size([31792, 64])\n",
            "article torch.Size([894, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "from torch import nn\n",
        "\n",
        "from dgl import function as fn\n",
        "from dgl.nn.functional import edge_softmax\n",
        "from dgl.base import DGLError\n",
        "from dgl.nn.pytorch.utils import Identity\n",
        "from dgl.utils import expand_as_pair\n",
        "\n",
        "\n",
        "class GATConv(nn.Module):\n",
        "    r\"\"\"\n",
        "\n",
        "    Description\n",
        "    -----------\n",
        "    Apply `Graph Attention Network <https://arxiv.org/pdf/1710.10903.pdf>`__\n",
        "    over an input signal.\n",
        "\n",
        "    .. math::\n",
        "        h_i^{(l+1)} = \\sum_{j\\in \\mathcal{N}(i)} \\alpha_{i,j} W^{(l)} h_j^{(l)}\n",
        "\n",
        "    where :math:`\\alpha_{ij}` is the attention score bewteen node :math:`i` and\n",
        "    node :math:`j`:\n",
        "\n",
        "    .. math::\n",
        "        \\alpha_{ij}^{l} &= \\mathrm{softmax_i} (e_{ij}^{l})\n",
        "\n",
        "        e_{ij}^{l} &= \\mathrm{LeakyReLU}\\left(\\vec{a}^T [W h_{i} \\| W h_{j}]\\right)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_feats : int, or pair of ints\n",
        "        Input feature size; i.e, the number of dimensions of :math:`h_i^{(l)}`.\n",
        "        GATConv can be applied on homogeneous graph and unidirectional\n",
        "        `bipartite graph <https://docs.dgl.ai/generated/dgl.bipartite.html?highlight=bipartite>`__.\n",
        "        If the layer is to be applied to a unidirectional bipartite graph, ``in_feats``\n",
        "        specifies the input feature size on both the source and destination nodes.  If\n",
        "        a scalar is given, the source and destination node feature size would take the\n",
        "        same value.\n",
        "    out_feats : int\n",
        "        Output feature size; i.e, the number of dimensions of :math:`h_i^{(l+1)}`.\n",
        "    num_heads : int\n",
        "        Number of heads in Multi-Head Attention.\n",
        "    feat_drop : float, optional\n",
        "        Dropout rate on feature. Defaults: ``0``.\n",
        "    attn_drop : float, optional\n",
        "        Dropout rate on attention weight. Defaults: ``0``.\n",
        "    negative_slope : float, optional\n",
        "        LeakyReLU angle of negative slope. Defaults: ``0.2``.\n",
        "    residual : bool, optional\n",
        "        If True, use residual connection. Defaults: ``False``.\n",
        "    activation : callable activation function/layer or None, optional.\n",
        "        If not None, applies an activation function to the updated node features.\n",
        "        Default: ``None``.\n",
        "    allow_zero_in_degree : bool, optional\n",
        "        If there are 0-in-degree nodes in the graph, output for those nodes will be invalid\n",
        "        since no message will be passed to those nodes. This is harmful for some applications\n",
        "        causing silent performance regression. This module will raise a DGLError if it detects\n",
        "        0-in-degree nodes in input graph. By setting ``True``, it will suppress the check\n",
        "        and let the users handle it by themselves. Defaults: ``False``.\n",
        "    bias : bool, optional\n",
        "        If True, learns a bias term. Defaults: ``True``.\n",
        "\n",
        "    Note\n",
        "    ----\n",
        "    Zero in-degree nodes will lead to invalid output value. This is because no message\n",
        "    will be passed to those nodes, the aggregation function will be appied on empty input.\n",
        "    A common practice to avoid this is to add a self-loop for each node in the graph if\n",
        "    it is homogeneous, which can be achieved by:\n",
        "\n",
        "    >>> g = ... # a DGLGraph\n",
        "    >>> g = dgl.add_self_loop(g)\n",
        "\n",
        "    Calling ``add_self_loop`` will not work for some graphs, for example, heterogeneous graph\n",
        "    since the edge type can not be decided for self_loop edges. Set ``allow_zero_in_degree``\n",
        "    to ``True`` for those cases to unblock the code and handle zero-in-degree nodes manually.\n",
        "    A common practise to handle this is to filter out the nodes with zero-in-degree when use\n",
        "    after conv.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import dgl\n",
        "    >>> import numpy as np\n",
        "    >>> import torch as th\n",
        "    >>> from dgl.nn import GATConv\n",
        "\n",
        "    >>> # Case 1: Homogeneous graph\n",
        "    >>> g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))\n",
        "    >>> g = dgl.add_self_loop(g)\n",
        "    >>> feat = th.ones(6, 10)\n",
        "    >>> gatconv = GATConv(10, 2, num_heads=3)\n",
        "    >>> res = gatconv(g, feat)\n",
        "    >>> res\n",
        "    tensor([[[ 3.4570,  1.8634],\n",
        "            [ 1.3805, -0.0762],\n",
        "            [ 1.0390, -1.1479]],\n",
        "            [[ 3.4570,  1.8634],\n",
        "            [ 1.3805, -0.0762],\n",
        "            [ 1.0390, -1.1479]],\n",
        "            [[ 3.4570,  1.8634],\n",
        "            [ 1.3805, -0.0762],\n",
        "            [ 1.0390, -1.1479]],\n",
        "            [[ 3.4570,  1.8634],\n",
        "            [ 1.3805, -0.0762],\n",
        "            [ 1.0390, -1.1479]],\n",
        "            [[ 3.4570,  1.8634],\n",
        "            [ 1.3805, -0.0762],\n",
        "            [ 1.0390, -1.1479]],\n",
        "            [[ 3.4570,  1.8634],\n",
        "            [ 1.3805, -0.0762],\n",
        "            [ 1.0390, -1.1479]]], grad_fn=<BinaryReduceBackward>)\n",
        "\n",
        "    >>> # Case 2: Unidirectional bipartite graph\n",
        "    >>> u = [0, 1, 0, 0, 1]\n",
        "    >>> v = [0, 1, 2, 3, 2]\n",
        "    >>> g = dgl.heterograph({('A', 'r', 'B'): (u, v)})\n",
        "    >>> u_feat = th.tensor(np.random.rand(2, 5).astype(np.float32))\n",
        "    >>> v_feat = th.tensor(np.random.rand(4, 10).astype(np.float32))\n",
        "    >>> gatconv = GATConv((5,10), 2, 3)\n",
        "    >>> res = gatconv(g, (u_feat, v_feat))\n",
        "    >>> res\n",
        "    tensor([[[-0.6066,  1.0268],\n",
        "            [-0.5945, -0.4801],\n",
        "            [ 0.1594,  0.3825]],\n",
        "            [[ 0.0268,  1.0783],\n",
        "            [ 0.5041, -1.3025],\n",
        "            [ 0.6568,  0.7048]],\n",
        "            [[-0.2688,  1.0543],\n",
        "            [-0.0315, -0.9016],\n",
        "            [ 0.3943,  0.5347]],\n",
        "            [[-0.6066,  1.0268],\n",
        "            [-0.5945, -0.4801],\n",
        "            [ 0.1594,  0.3825]]], grad_fn=<BinaryReduceBackward>)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 out_feats,\n",
        "                 num_heads,\n",
        "                 feat_drop=0.,\n",
        "                 attn_drop=0.,\n",
        "                 negative_slope=0.2,\n",
        "                 residual=False,\n",
        "                 activation=None,\n",
        "                 allow_zero_in_degree=False,\n",
        "                 bias=True):\n",
        "        super(GATConv, self).__init__()\n",
        "        self._num_heads = num_heads\n",
        "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
        "        self._out_feats = out_feats\n",
        "        self._allow_zero_in_degree = allow_zero_in_degree\n",
        "        if isinstance(in_feats, tuple):\n",
        "            self.fc_src = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "            self.fc_dst = nn.Linear(\n",
        "                self._in_dst_feats, out_feats * num_heads, bias=False)\n",
        "        else:\n",
        "            self.fc = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(th.FloatTensor(size=(num_heads * out_feats,)))\n",
        "        else:\n",
        "            self.register_buffer('bias', None)\n",
        "        if residual:\n",
        "            if self._in_dst_feats != out_feats * num_heads:\n",
        "                self.res_fc = nn.Linear(\n",
        "                    self._in_dst_feats, num_heads * out_feats, bias=False)\n",
        "            else:\n",
        "                self.res_fc = Identity()\n",
        "        else:\n",
        "            self.register_buffer('res_fc', None)\n",
        "        self.reset_parameters()\n",
        "        self.activation = activation\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"\n",
        "\n",
        "        Description\n",
        "        -----------\n",
        "        Reinitialize learnable parameters.\n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        The fc weights :math:`W^{(l)}` are initialized using Glorot uniform initialization.\n",
        "        The attention weights are using xavier initialization method.\n",
        "        \"\"\"\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        if hasattr(self, 'fc'):\n",
        "            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "        else:\n",
        "            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n",
        "            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_l, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_r, gain=gain)\n",
        "        if self.bias is not None:\n",
        "            nn.init.constant_(self.bias, 0)\n",
        "        if isinstance(self.res_fc, nn.Linear):\n",
        "            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n",
        "\n",
        "    def set_allow_zero_in_degree(self, set_value):\n",
        "        r\"\"\"\n",
        "\n",
        "        Description\n",
        "        -----------\n",
        "        Set allow_zero_in_degree flag.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        set_value : bool\n",
        "            The value to be set to the flag.\n",
        "        \"\"\"\n",
        "        self._allow_zero_in_degree = set_value\n",
        "\n",
        "    def forward(self, graph, feat, get_attention=False):\n",
        "        r\"\"\"\n",
        "\n",
        "        Description\n",
        "        -----------\n",
        "        Compute graph attention network layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : DGLGraph\n",
        "            The graph.\n",
        "        feat : torch.Tensor or pair of torch.Tensor\n",
        "            If a torch.Tensor is given, the input feature of shape :math:`(N, *, D_{in})` where\n",
        "            :math:`D_{in}` is size of input feature, :math:`N` is the number of nodes.\n",
        "            If a pair of torch.Tensor is given, the pair must contain two tensors of shape\n",
        "            :math:`(N_{in}, *, D_{in_{src}})` and :math:`(N_{out}, *, D_{in_{dst}})`.\n",
        "        get_attention : bool, optional\n",
        "            Whether to return the attention values. Default to False.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The output feature of shape :math:`(N, *, H, D_{out})` where :math:`H`\n",
        "            is the number of heads, and :math:`D_{out}` is size of output feature.\n",
        "        torch.Tensor, optional\n",
        "            The attention values of shape :math:`(E, *, H, 1)`, where :math:`E` is the number of\n",
        "            edges. This is returned only when :attr:`get_attention` is ``True``.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        DGLError\n",
        "            If there are 0-in-degree nodes in the input graph, it will raise DGLError\n",
        "            since no message will be passed to those nodes. This will cause invalid output.\n",
        "            The error can be ignored by setting ``allow_zero_in_degree`` parameter to ``True``.\n",
        "        \"\"\"\n",
        "        with graph.local_scope():\n",
        "            if not self._allow_zero_in_degree:\n",
        "                if (graph.in_degrees() == 0).any():\n",
        "                    raise DGLError('There are 0-in-degree nodes in the graph, '\n",
        "                                   'output for those nodes will be invalid. '\n",
        "                                   'This is harmful for some applications, '\n",
        "                                   'causing silent performance regression. '\n",
        "                                   'Adding self-loop on the input graph by '\n",
        "                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n",
        "                                   'the issue. Setting ``allow_zero_in_degree`` '\n",
        "                                   'to be `True` when constructing this module will '\n",
        "                                   'suppress the check and let the code run.')\n",
        "\n",
        "            if isinstance(feat, tuple):\n",
        "                src_prefix_shape = feat[0].shape[:-1]\n",
        "                dst_prefix_shape = feat[1].shape[:-1]\n",
        "                h_src = self.feat_drop(feat[0])\n",
        "                h_dst = self.feat_drop(feat[1])\n",
        "                if not hasattr(self, 'fc_src'):\n",
        "                    feat_src = self.fc(h_src).view(\n",
        "                        *src_prefix_shape, self._num_heads, self._out_feats)\n",
        "                    feat_dst = self.fc(h_dst).view(\n",
        "                        *dst_prefix_shape, self._num_heads, self._out_feats)\n",
        "                else:\n",
        "                    feat_src = self.fc_src(h_src).view(\n",
        "                        *src_prefix_shape, self._num_heads, self._out_feats)\n",
        "                    feat_dst = self.fc_dst(h_dst).view(\n",
        "                        *dst_prefix_shape, self._num_heads, self._out_feats)\n",
        "            else:\n",
        "                src_prefix_shape = dst_prefix_shape = feat.shape[:-1]\n",
        "                h_src = h_dst = self.feat_drop(feat)\n",
        "                feat_src = feat_dst = self.fc(h_src).view(\n",
        "                    *src_prefix_shape, self._num_heads, self._out_feats)\n",
        "                if graph.is_block:\n",
        "                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n",
        "                    h_dst = h_dst[:graph.number_of_dst_nodes()]\n",
        "                    dst_prefix_shape = (graph.number_of_dst_nodes(),) + dst_prefix_shape[1:]\n",
        "            # NOTE: GAT paper uses \"first concatenation then linear projection\"\n",
        "            # to compute attention scores, while ours is \"first projection then\n",
        "            # addition\", the two approaches are mathematically equivalent:\n",
        "            # We decompose the weight vector a mentioned in the paper into\n",
        "            # [a_l || a_r], then\n",
        "            # a^T [Wh_i || Wh_j] = a_l Wh_i + a_r Wh_j\n",
        "            # Our implementation is much efficient because we do not need to\n",
        "            # save [Wh_i || Wh_j] on edges, which is not memory-efficient. Plus,\n",
        "            # addition could be optimized with DGL's built-in function u_add_v,\n",
        "            # which further speeds up computation and saves memory footprint.\n",
        "            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n",
        "            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n",
        "            graph.srcdata.update({'ft': feat_src, 'el': el})\n",
        "            graph.dstdata.update({'er': er})\n",
        "            # compute edge attention, el and er are a_l Wh_i and a_r Wh_j respectively.\n",
        "            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n",
        "            e = self.leaky_relu(graph.edata.pop('e'))\n",
        "            # compute softmax\n",
        "            graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n",
        "            # message passing\n",
        "            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n",
        "                             fn.sum('m', 'ft'))\n",
        "            rst = graph.dstdata['ft']\n",
        "            # residual\n",
        "            if self.res_fc is not None:\n",
        "                # Use -1 rather than self._num_heads to handle broadcasting\n",
        "                resval = self.res_fc(h_dst).view(*dst_prefix_shape, self._num_heads, self._out_feats)\n",
        "                rst = rst + resval\n",
        "            # bias\n",
        "            if self.bias is not None:\n",
        "                rst = rst + self.bias.view(\n",
        "                    *((1,) * len(dst_prefix_shape)), self._num_heads, self._out_feats)\n",
        "            # activation\n",
        "            if self.activation:\n",
        "                rst = self.activation(rst)\n",
        "\n",
        "            if get_attention:\n",
        "                return rst, graph.edata['a']\n",
        "            else:\n",
        "                return rst\n"
      ],
      "metadata": {
        "id": "k82o39yKiI6l"
      },
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATEncoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_h, etypes, num_heads=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.conv1 = dgl.nn.HeteroGraphConv({\n",
        "            rel : GATConv(d_in, d_h, num_heads, dropout, dropout,\n",
        "                      residual=True, activation=nn.SELU(), \n",
        "                  ) for rel in etypes\n",
        "        })\n",
        "        self.conv2 = dgl.nn.HeteroGraphConv({\n",
        "            rel : GATConv(d_h*num_heads, d_h, num_heads, dropout, dropout,\n",
        "                      residual=True, activation=None, \n",
        "                  ) for rel in etypes\n",
        "        })\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        x = flatten_dict(self.conv1(graph, x))\n",
        "        x = flatten_dict(self.conv2(graph, x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "VFx3l8mOsuke"
      },
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hetero_graph.etypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD2XRKbc1y05",
        "outputId": "a784033a-97d6-4d0b-ed47-5dad79b822ff"
      },
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tweeted-by', 'follow', 'followed-by', 'tweet']"
            ]
          },
          "metadata": {},
          "execution_count": 329
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc = GATEncoder(64, 64, hetero_graph.etypes)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = enc(hetero_graph, x)\n",
        "\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qTEeMWt1NUq",
        "outputId": "d5a2c621-cc99-4e32-8e6f-9bcf6adbd03d"
      },
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'article': tensor([[-8.6927e-01,  8.9977e-01,  4.5092e-01,  ...,  1.0683e+00,\n",
              "           0.0000e+00, -1.1636e+00],\n",
              "         [-1.0188e+00,  1.1493e+00,  0.0000e+00,  ...,  1.2287e+00,\n",
              "          -1.9891e-01, -1.3664e+00],\n",
              "         [-6.3096e-01,  0.0000e+00,  8.7443e-01,  ...,  1.4495e+00,\n",
              "           5.9866e-02, -1.2981e+00],\n",
              "         ...,\n",
              "         [ 3.4970e-01, -8.5775e-01, -1.0827e+00,  ..., -1.3127e+00,\n",
              "          -3.2055e-01,  9.7906e-01],\n",
              "         [-4.7286e-01,  9.2204e-01,  7.6464e-01,  ...,  8.8718e-01,\n",
              "          -2.1716e-01,  0.0000e+00],\n",
              "         [ 9.0053e+03,  9.0819e+03,  1.1882e+04,  ..., -7.0525e+03,\n",
              "           1.3924e+03,  1.2293e+02]]),\n",
              " 'user': tensor([[-7.1073e+03,  2.2604e+03,  7.2117e+02,  ..., -9.5134e+03,\n",
              "           3.1511e+04,  4.6570e+04],\n",
              "         [ 1.2061e+04,  9.7785e+02,  1.4455e+04,  ..., -2.0080e+02,\n",
              "          -1.7169e+04,  9.2001e+03],\n",
              "         [ 1.2061e+04,  9.7790e+02,  1.4455e+04,  ..., -2.0085e+02,\n",
              "          -1.7169e+04,  9.1998e+03],\n",
              "         ...,\n",
              "         [ 7.1512e+03,  5.2286e+03, -1.0619e+04,  ..., -4.1115e+01,\n",
              "           9.2854e+03, -1.9709e+03],\n",
              "         [ 7.1511e+03,  5.2286e+03, -1.0619e+04,  ..., -4.1233e+01,\n",
              "           9.2854e+03, -1.9709e+03],\n",
              "         [ 7.1514e+03,  5.2286e+03, -1.0619e+04,  ..., -4.1318e+01,\n",
              "           9.2851e+03, -1.9712e+03]])}"
            ]
          },
          "metadata": {},
          "execution_count": 330
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HetGNN(nn.Module):\n",
        "\n",
        "    def __init__(self, g, n_nodes, d_feats, d_emb, d_h, tgt_ntype, num_heads=1):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(123)\n",
        "        self.tgt_ntype = tgt_ntype\n",
        "        self.emb = NodeEmbedding(n_nodes, d_feats, d_emb)\n",
        "        self.encoder = GATEncoder(d_emb, d_h, g.etypes, num_heads)\n",
        "        self.head = nn.Linear(d_h*num_heads, 2)\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        h = self.emb(x)\n",
        "        h = self.encoder(graph, h)\n",
        "        return self.head(h[self.tgt_ntype])"
      ],
      "metadata": {
        "id": "QKkjZgvfzZbn"
      },
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HetGNN(hetero_graph, {k:hetero_graph.num_nodes(k) for k in [\"user\"]}, {'article':text_embs.shape[1]}, 64, 64, \"article\")"
      ],
      "metadata": {
        "id": "aY0DeDcd3jy-"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    x = hetero_graph.ndata['feat']\n",
        "    logits = model(hetero_graph, x)\n",
        "\n",
        "logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz8m3W6l4S6J",
        "outputId": "c843b4e2-c03d-456c-a136-37efbe555a13"
      },
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.9343e-01,  4.2438e-01],\n",
              "        [-2.4458e-01, -2.2428e-02],\n",
              "        [ 2.5643e-01, -1.3029e-01],\n",
              "        ...,\n",
              "        [ 1.9467e-01,  9.6576e-02],\n",
              "        [ 2.2322e-01,  1.2810e-01],\n",
              "        [-4.1827e+03,  8.0900e+03]])"
            ]
          },
          "metadata": {},
          "execution_count": 333
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(logits, labels):\n",
        "    return (logits.argmax(-1) == labels).float().mean()"
      ],
      "metadata": {
        "id": "BunQmwFY8Xso"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HetGNN(hetero_graph, {k:hetero_graph.num_nodes(k) for k in [\"user\"]}, {'article':text_embs.shape[1]}, 64, 64, \"article\")\n",
        "\n",
        "x = hetero_graph.ndata['feat']\n",
        "train_mask = hetero_graph.ndata['train_mask']['article']\n",
        "labels = hetero_graph.ndata['label']['article']"
      ],
      "metadata": {
        "id": "yvbuuvi556U4"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "for epoch in range(40):\n",
        "    model.train()\n",
        "    logits = model(hetero_graph, x)\n",
        "\n",
        "    print(logits)\n",
        "    \n",
        "    loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
        "    acc = accuracy(logits[train_mask], labels[train_mask])*100\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_loss = F.cross_entropy(logits[~train_mask], labels[~train_mask])\n",
        "        val_acc = accuracy(logits[~train_mask], labels[~train_mask])*100\n",
        "    print(f\"{epoch+1:>2}: Train loss {loss.item():.4f}, acc {acc:.2f}%; validation loss {val_loss.item():.4f}, acc {val_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kiu8dH-42Xay",
        "outputId": "4e4093c0-3956-4c45-fbbf-ae62cccb0d68"
      },
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.9343e-01,  4.2438e-01],\n",
            "        [-2.4458e-01, -2.2428e-02],\n",
            "        [ 2.5643e-01, -1.3029e-01],\n",
            "        ...,\n",
            "        [ 1.9467e-01,  9.6576e-02],\n",
            "        [ 2.2322e-01,  1.2810e-01],\n",
            "        [-4.1827e+03,  8.0900e+03]], grad_fn=<AddmmBackward0>)\n",
            " 1: Train loss 4.4912, acc 49.24%; validation loss 0.6235, acc 49.71%\n",
            "tensor([[ 1.0228e+00, -8.5557e-01],\n",
            "        [ 1.1809e+00, -8.6816e-01],\n",
            "        [ 1.0469e+00, -9.2738e-01],\n",
            "        ...,\n",
            "        [-8.3437e-01,  8.1696e-01],\n",
            "        [ 1.1226e+00, -8.9444e-01],\n",
            "        [-9.2805e+02,  1.0687e+04]], grad_fn=<AddmmBackward0>)\n",
            " 2: Train loss 0.1649, acc 96.82%; validation loss 0.1443, acc 98.25%\n",
            "tensor([[ 1.9027e+00, -1.8078e+00],\n",
            "        [ 2.0474e+00, -1.8245e+00],\n",
            "        [ 2.3304e+00, -1.0493e+00],\n",
            "        ...,\n",
            "        [-1.7957e+00,  2.0808e+00],\n",
            "        [ 2.0142e+00, -1.4106e+00],\n",
            "        [-5.0123e+03,  1.0796e+04]], grad_fn=<AddmmBackward0>)\n",
            " 3: Train loss 0.1012, acc 96.68%; validation loss 0.0724, acc 98.25%\n",
            "tensor([[ 2.3414e+00, -1.8088e+00],\n",
            "        [ 3.2008e+00, -3.1791e+00],\n",
            "        [ 3.1406e+00, -2.3051e+00],\n",
            "        ...,\n",
            "        [-2.2605e+00,  2.6444e+00],\n",
            "        [ 2.6792e+00, -2.5882e+00],\n",
            "        [-1.0093e+03,  8.9187e+03]], grad_fn=<AddmmBackward0>)\n",
            " 4: Train loss 5.7924, acc 96.54%; validation loss 0.0552, acc 98.25%\n",
            "tensor([[ 2.2086e+00, -1.9620e+00],\n",
            "        [ 3.9359e+00, -2.5876e+00],\n",
            "        [ 3.4129e+00, -2.5745e+00],\n",
            "        ...,\n",
            "        [-2.8964e+00,  2.6131e+00],\n",
            "        [ 2.6940e+00, -1.9857e+00],\n",
            "        [-6.8345e+03,  9.9772e+03]], grad_fn=<AddmmBackward0>)\n",
            " 5: Train loss 0.0922, acc 96.96%; validation loss 0.0516, acc 98.25%\n",
            "tensor([[ 2.8007e+00, -2.5241e+00],\n",
            "        [ 4.0643e+00, -3.7445e+00],\n",
            "        [ 3.3524e+00, -2.9611e+00],\n",
            "        ...,\n",
            "        [-2.2940e+00,  2.8173e+00],\n",
            "        [ 3.3858e+00, -3.0214e+00],\n",
            "        [-1.4176e+04,  2.0568e+04]], grad_fn=<AddmmBackward0>)\n",
            " 6: Train loss 0.0937, acc 96.82%; validation loss 0.0508, acc 98.25%\n",
            "tensor([[ 3.1094e+00, -2.7788e+00],\n",
            "        [ 3.5910e+00, -3.2022e+00],\n",
            "        [ 3.5077e+00, -2.4089e+00],\n",
            "        ...,\n",
            "        [-3.8087e+00,  3.1225e+00],\n",
            "        [ 2.6976e+00, -3.1720e+00],\n",
            "        [-2.1496e+04,  2.3568e+04]], grad_fn=<AddmmBackward0>)\n",
            " 7: Train loss 0.0913, acc 96.96%; validation loss 0.0528, acc 97.66%\n",
            "tensor([[ 2.8668e+00, -2.4126e+00],\n",
            "        [ 4.0806e+00, -3.5578e+00],\n",
            "        [ 3.5060e+00, -3.2642e+00],\n",
            "        ...,\n",
            "        [-3.8259e+00,  2.5094e+00],\n",
            "        [ 3.2501e+00, -2.6104e+00],\n",
            "        [-2.2050e+04,  3.0346e+04]], grad_fn=<AddmmBackward0>)\n",
            " 8: Train loss 0.0911, acc 96.82%; validation loss 0.0509, acc 97.66%\n",
            "tensor([[ 2.0250e+00, -2.1694e+00],\n",
            "        [ 3.4709e+00, -3.6108e+00],\n",
            "        [ 3.8857e+00, -3.0432e+00],\n",
            "        ...,\n",
            "        [-3.9841e+00,  3.6475e+00],\n",
            "        [ 3.7677e+00, -2.9621e+00],\n",
            "        [-2.6742e+04,  3.4808e+04]], grad_fn=<AddmmBackward0>)\n",
            " 9: Train loss 0.0951, acc 96.96%; validation loss 0.0534, acc 98.25%\n",
            "tensor([[ 3.0162e+00, -2.8445e+00],\n",
            "        [ 4.2227e+00, -3.2526e+00],\n",
            "        [ 3.8818e+00, -3.2366e+00],\n",
            "        ...,\n",
            "        [-3.8955e+00,  4.1615e+00],\n",
            "        [ 2.4457e+00, -3.4771e+00],\n",
            "        [-2.2369e+04,  2.4797e+04]], grad_fn=<AddmmBackward0>)\n",
            "10: Train loss 0.0923, acc 96.82%; validation loss 0.0565, acc 97.66%\n",
            "tensor([[ 2.8735e+00, -3.2642e+00],\n",
            "        [ 3.6453e+00, -3.6091e+00],\n",
            "        [ 3.5015e+00, -1.8205e+00],\n",
            "        ...,\n",
            "        [-4.5125e+00,  2.9709e+00],\n",
            "        [ 3.4840e+00, -3.1742e+00],\n",
            "        [-3.4477e+04,  3.9347e+04]], grad_fn=<AddmmBackward0>)\n",
            "11: Train loss 0.0955, acc 96.96%; validation loss 0.0516, acc 97.66%\n",
            "tensor([[ 2.0861e+00, -2.4164e+00],\n",
            "        [ 4.0368e+00, -4.1443e+00],\n",
            "        [ 3.2738e+00, -2.8677e+00],\n",
            "        ...,\n",
            "        [-4.8639e+00,  3.5711e+00],\n",
            "        [ 3.5440e+00, -3.1393e+00],\n",
            "        [-3.9644e+04,  4.2728e+04]], grad_fn=<AddmmBackward0>)\n",
            "12: Train loss 0.0959, acc 96.82%; validation loss 0.0525, acc 97.66%\n",
            "tensor([[ 2.7113e+00, -2.1827e+00],\n",
            "        [ 3.5654e+00, -3.8023e+00],\n",
            "        [ 3.8887e+00, -3.3398e+00],\n",
            "        ...,\n",
            "        [-4.6210e+00,  3.1936e+00],\n",
            "        [ 3.8139e+00, -2.8199e+00],\n",
            "        [-3.8293e+04,  3.9430e+04]], grad_fn=<AddmmBackward0>)\n",
            "13: Train loss 0.0920, acc 96.82%; validation loss 0.0511, acc 97.66%\n",
            "tensor([[ 3.0679e+00, -3.1108e+00],\n",
            "        [ 3.5259e+00, -3.7081e+00],\n",
            "        [ 3.8331e+00, -3.4065e+00],\n",
            "        ...,\n",
            "        [-4.8032e+00,  3.3910e+00],\n",
            "        [ 3.4550e+00, -2.2249e+00],\n",
            "        [-3.8386e+04,  4.1242e+04]], grad_fn=<AddmmBackward0>)\n",
            "14: Train loss 0.0919, acc 96.82%; validation loss 0.0561, acc 97.66%\n",
            "tensor([[ 3.0434e+00, -3.0981e+00],\n",
            "        [ 3.7345e+00, -4.7888e+00],\n",
            "        [ 3.4554e+00, -2.6881e+00],\n",
            "        ...,\n",
            "        [-5.0084e+00,  4.0150e+00],\n",
            "        [ 2.9152e+00, -3.1551e+00],\n",
            "        [-5.1224e+04,  5.4853e+04]], grad_fn=<AddmmBackward0>)\n",
            "15: Train loss 0.0888, acc 96.82%; validation loss 0.0536, acc 97.66%\n",
            "tensor([[ 3.3301e+00, -3.0395e+00],\n",
            "        [ 4.5782e+00, -3.9643e+00],\n",
            "        [ 4.1199e+00, -3.5850e+00],\n",
            "        ...,\n",
            "        [-4.6469e+00,  3.7882e+00],\n",
            "        [ 3.5658e+00, -2.5975e+00],\n",
            "        [-3.9237e+03,  4.8010e+03]], grad_fn=<AddmmBackward0>)\n",
            "16: Train loss 0.0867, acc 96.82%; validation loss 0.0552, acc 97.66%\n",
            "tensor([[ 3.9211e+00, -2.7532e+00],\n",
            "        [ 4.0882e+00, -3.5939e+00],\n",
            "        [ 3.3006e+00, -2.7654e+00],\n",
            "        ...,\n",
            "        [-4.9179e+00,  3.0494e+00],\n",
            "        [ 2.7273e+00, -2.0414e+00],\n",
            "        [-4.1810e+04,  4.3764e+04]], grad_fn=<AddmmBackward0>)\n",
            "17: Train loss 0.0862, acc 96.82%; validation loss 0.0583, acc 97.66%\n",
            "tensor([[ 2.8516e+00, -2.6493e+00],\n",
            "        [ 4.2039e+00, -3.9939e+00],\n",
            "        [ 4.7751e+00, -3.0667e+00],\n",
            "        ...,\n",
            "        [-5.4791e+00,  3.9732e+00],\n",
            "        [ 2.5558e+00, -2.8028e+00],\n",
            "        [-4.6456e+04,  4.7491e+04]], grad_fn=<AddmmBackward0>)\n",
            "18: Train loss 0.0890, acc 96.96%; validation loss 0.0571, acc 97.66%\n",
            "tensor([[ 3.4922e+00, -2.8635e+00],\n",
            "        [ 4.6266e+00, -3.8252e+00],\n",
            "        [ 3.6618e+00, -3.2531e+00],\n",
            "        ...,\n",
            "        [-5.5154e+00,  3.5762e+00],\n",
            "        [ 3.6739e+00, -2.9130e+00],\n",
            "        [-4.3991e+04,  5.0435e+04]], grad_fn=<AddmmBackward0>)\n",
            "19: Train loss 0.0843, acc 96.82%; validation loss 0.0564, acc 97.66%\n",
            "tensor([[ 2.8952e+00, -3.6319e+00],\n",
            "        [ 3.9060e+00, -2.8698e+00],\n",
            "        [ 4.0112e+00, -2.0335e+00],\n",
            "        ...,\n",
            "        [-5.8233e+00,  3.5594e+00],\n",
            "        [ 3.4797e+00, -3.3288e+00],\n",
            "        [-5.4039e+04,  5.7582e+04]], grad_fn=<AddmmBackward0>)\n",
            "20: Train loss 0.0811, acc 96.96%; validation loss 0.0587, acc 97.66%\n",
            "tensor([[ 2.2341e+00, -3.4136e+00],\n",
            "        [ 3.3752e+00, -4.6437e+00],\n",
            "        [ 3.8185e+00, -3.0734e+00],\n",
            "        ...,\n",
            "        [-4.8210e+00,  3.6372e+00],\n",
            "        [ 3.5659e+00, -3.6175e+00],\n",
            "        [-5.0664e+03,  5.8924e+03]], grad_fn=<AddmmBackward0>)\n",
            "21: Train loss 0.0795, acc 96.82%; validation loss 0.0605, acc 97.66%\n",
            "tensor([[ 1.9848e+00, -2.1615e+00],\n",
            "        [ 4.1388e+00, -4.5747e+00],\n",
            "        [ 2.6191e+00, -3.4055e+00],\n",
            "        ...,\n",
            "        [-5.2153e+00,  3.8299e+00],\n",
            "        [ 3.6287e+00, -3.1061e+00],\n",
            "        [-5.0798e+04,  4.6447e+04]], grad_fn=<AddmmBackward0>)\n",
            "22: Train loss 0.0779, acc 97.10%; validation loss 0.0600, acc 97.66%\n",
            "tensor([[ 2.6165e+00, -2.0728e+00],\n",
            "        [ 4.2710e+00, -4.0837e+00],\n",
            "        [ 3.7848e+00, -3.2166e+00],\n",
            "        ...,\n",
            "        [-5.4654e+00,  4.2629e+00],\n",
            "        [ 3.0719e+00, -3.3330e+00],\n",
            "        [-6.6182e+04,  6.4694e+04]], grad_fn=<AddmmBackward0>)\n",
            "23: Train loss 0.0780, acc 97.23%; validation loss 0.0620, acc 97.66%\n",
            "tensor([[ 2.7479e+00, -2.7676e+00],\n",
            "        [ 3.6398e+00, -3.6893e+00],\n",
            "        [ 3.1877e+00, -2.8891e+00],\n",
            "        ...,\n",
            "        [-5.1252e+00,  2.7555e+00],\n",
            "        [ 4.0455e+00, -3.2699e+00],\n",
            "        [-6.3986e+04,  6.6140e+04]], grad_fn=<AddmmBackward0>)\n",
            "24: Train loss 0.0796, acc 97.23%; validation loss 0.0587, acc 97.66%\n",
            "tensor([[ 3.7651e+00, -2.8394e+00],\n",
            "        [ 4.1637e+00, -3.9210e+00],\n",
            "        [ 3.4914e+00, -3.2741e+00],\n",
            "        ...,\n",
            "        [-3.6308e+00,  2.8836e+00],\n",
            "        [ 3.8675e+00, -3.0389e+00],\n",
            "        [-6.4029e+04,  6.5883e+04]], grad_fn=<AddmmBackward0>)\n",
            "25: Train loss 0.0739, acc 97.23%; validation loss 0.0594, acc 97.66%\n",
            "tensor([[ 2.3628e+00, -2.0576e+00],\n",
            "        [ 3.3847e+00, -3.0081e+00],\n",
            "        [ 3.5781e+00, -2.8284e+00],\n",
            "        ...,\n",
            "        [-5.3798e+00,  2.9304e+00],\n",
            "        [ 3.1418e+00, -3.6649e+00],\n",
            "        [-7.1570e+04,  7.2056e+04]], grad_fn=<AddmmBackward0>)\n",
            "26: Train loss 0.0733, acc 97.37%; validation loss 0.0619, acc 97.66%\n",
            "tensor([[ 3.4027e+00, -3.4841e+00],\n",
            "        [ 3.6983e+00, -4.8720e+00],\n",
            "        [ 2.8276e+00, -2.7521e+00],\n",
            "        ...,\n",
            "        [-5.4374e+00,  3.8851e+00],\n",
            "        [ 3.5151e+00, -4.1235e+00],\n",
            "        [-5.9339e+04,  6.0652e+04]], grad_fn=<AddmmBackward0>)\n",
            "27: Train loss 0.0679, acc 97.37%; validation loss 0.0627, acc 97.66%\n",
            "tensor([[ 3.4956e+00, -3.2779e+00],\n",
            "        [ 3.5218e+00, -4.3705e+00],\n",
            "        [ 3.9910e+00, -3.0472e+00],\n",
            "        ...,\n",
            "        [-4.4613e+00,  3.1266e+00],\n",
            "        [ 3.9469e+00, -3.6925e+00],\n",
            "        [-3.3391e+03,  3.8761e+03]], grad_fn=<AddmmBackward0>)\n",
            "28: Train loss 0.0703, acc 97.51%; validation loss 0.0666, acc 97.66%\n",
            "tensor([[ 3.1478e+00, -3.2424e+00],\n",
            "        [ 3.1638e+00, -4.0337e+00],\n",
            "        [ 4.1112e+00, -2.7054e+00],\n",
            "        ...,\n",
            "        [-4.9034e+00,  3.1330e+00],\n",
            "        [ 3.6408e+00, -3.5673e+00],\n",
            "        [-6.1736e+04,  6.1813e+04]], grad_fn=<AddmmBackward0>)\n",
            "29: Train loss 0.0666, acc 97.51%; validation loss 0.0666, acc 97.66%\n",
            "tensor([[ 2.3927e+00, -2.5115e+00],\n",
            "        [ 3.5801e+00, -3.3722e+00],\n",
            "        [ 3.0020e+00, -2.9523e+00],\n",
            "        ...,\n",
            "        [-4.7548e+00,  2.8844e+00],\n",
            "        [ 3.2795e+00, -2.6650e+00],\n",
            "        [-6.5269e+04,  6.1967e+04]], grad_fn=<AddmmBackward0>)\n",
            "30: Train loss 0.0669, acc 97.79%; validation loss 0.0630, acc 97.66%\n",
            "tensor([[ 2.0480e+00, -2.9629e+00],\n",
            "        [ 3.7711e+00, -2.7726e+00],\n",
            "        [ 2.8262e+00, -2.9191e+00],\n",
            "        ...,\n",
            "        [-4.4563e+00,  3.3698e+00],\n",
            "        [ 2.9370e+00, -2.5830e+00],\n",
            "        [-5.3351e+04,  6.0032e+04]], grad_fn=<AddmmBackward0>)\n",
            "31: Train loss 0.0652, acc 97.79%; validation loss 0.0608, acc 98.25%\n",
            "tensor([[ 2.5976e+00, -2.8859e+00],\n",
            "        [ 3.4352e+00, -3.4187e+00],\n",
            "        [ 3.1952e+00, -2.4463e+00],\n",
            "        ...,\n",
            "        [-4.0826e+00,  3.5159e+00],\n",
            "        [ 4.3842e+00, -3.4025e+00],\n",
            "        [-7.3807e+04,  7.4815e+04]], grad_fn=<AddmmBackward0>)\n",
            "32: Train loss 0.0626, acc 97.79%; validation loss 0.0625, acc 98.25%\n",
            "tensor([[ 2.3505e+00, -2.2871e+00],\n",
            "        [ 3.2947e+00, -3.6181e+00],\n",
            "        [ 2.0163e+00, -3.1178e+00],\n",
            "        ...,\n",
            "        [-3.9567e+00,  3.0596e+00],\n",
            "        [ 3.3907e+00, -3.1399e+00],\n",
            "        [-6.4012e+04,  7.2887e+04]], grad_fn=<AddmmBackward0>)\n",
            "33: Train loss 0.0616, acc 97.93%; validation loss 0.0680, acc 98.25%\n",
            "tensor([[ 3.1389e+00, -2.7157e+00],\n",
            "        [ 3.6966e+00, -3.8176e+00],\n",
            "        [ 2.9879e+00, -3.2933e+00],\n",
            "        ...,\n",
            "        [-3.4080e+00,  2.6584e+00],\n",
            "        [ 3.5717e+00, -3.3844e+00],\n",
            "        [-3.7511e+04,  3.6938e+04]], grad_fn=<AddmmBackward0>)\n",
            "34: Train loss 0.0576, acc 97.93%; validation loss 0.0669, acc 98.25%\n",
            "tensor([[ 3.2002e+00, -1.7730e+00],\n",
            "        [ 3.3620e+00, -4.0013e+00],\n",
            "        [ 2.0152e+00, -2.9158e+00],\n",
            "        ...,\n",
            "        [-5.0530e+00,  3.2380e+00],\n",
            "        [ 2.5624e+00, -2.5633e+00],\n",
            "        [-8.1206e+04,  7.8713e+04]], grad_fn=<AddmmBackward0>)\n",
            "35: Train loss 0.0569, acc 98.20%; validation loss 0.0676, acc 98.25%\n",
            "tensor([[ 2.1311e+00, -2.7478e+00],\n",
            "        [ 2.4880e+00, -3.6569e+00],\n",
            "        [ 2.7372e+00, -3.0721e+00],\n",
            "        ...,\n",
            "        [-4.6389e+00,  3.1430e+00],\n",
            "        [ 4.0025e+00, -3.6099e+00],\n",
            "        [-7.6043e+04,  7.2665e+04]], grad_fn=<AddmmBackward0>)\n",
            "36: Train loss 0.0561, acc 98.20%; validation loss 0.0746, acc 97.66%\n",
            "tensor([[ 2.7681e+00, -2.8269e+00],\n",
            "        [ 2.9984e+00, -3.5637e+00],\n",
            "        [ 2.2808e+00, -2.9639e+00],\n",
            "        ...,\n",
            "        [-4.8229e+00,  2.3283e+00],\n",
            "        [ 2.5381e+00, -2.8867e+00],\n",
            "        [-7.6731e+04,  7.6531e+04]], grad_fn=<AddmmBackward0>)\n",
            "37: Train loss 0.0546, acc 98.34%; validation loss 0.0700, acc 98.25%\n",
            "tensor([[ 3.3931e+00, -2.8218e+00],\n",
            "        [ 2.7363e+00, -2.9621e+00],\n",
            "        [ 2.7813e+00, -2.6672e+00],\n",
            "        ...,\n",
            "        [-4.5148e+00,  3.1526e+00],\n",
            "        [ 2.8620e+00, -3.2322e+00],\n",
            "        [-7.3510e+04,  7.4898e+04]], grad_fn=<AddmmBackward0>)\n",
            "38: Train loss 0.0540, acc 98.34%; validation loss 0.0695, acc 98.25%\n",
            "tensor([[ 2.0345e+00, -2.7674e+00],\n",
            "        [ 4.2301e+00, -3.4588e+00],\n",
            "        [ 3.1768e+00, -3.0753e+00],\n",
            "        ...,\n",
            "        [-4.2420e+00,  2.9292e+00],\n",
            "        [ 3.5844e+00, -3.5104e+00],\n",
            "        [-7.2121e+04,  7.9918e+04]], grad_fn=<AddmmBackward0>)\n",
            "39: Train loss 0.0507, acc 98.06%; validation loss 0.0679, acc 97.66%\n",
            "tensor([[ 2.7121e+00, -3.3345e+00],\n",
            "        [ 2.4039e+00, -3.4306e+00],\n",
            "        [ 3.1372e+00, -2.6373e+00],\n",
            "        ...,\n",
            "        [-4.6475e+00,  2.3408e+00],\n",
            "        [ 2.8793e+00, -3.0466e+00],\n",
            "        [-6.8321e+04,  7.3912e+04]], grad_fn=<AddmmBackward0>)\n",
            "40: Train loss 0.0478, acc 98.48%; validation loss 0.0693, acc 98.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_data = pd.DataFrame(text_embs)"
      ],
      "metadata": {
        "id": "J3G9DxAdFyiB"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_data\n"
      ],
      "metadata": {
        "id": "_tVEa3L9CmE0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "3d52caaa-2ada-46c1-ebe8-b660b155e2b9"
      },
      "execution_count": 338,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                0             1           0.1           1.1        0.1.1  \\\n",
              "0        1.349461     -2.602859      2.833252     -2.743249     6.893370   \n",
              "1        3.731273     -4.053496      2.255937     -2.753675     7.221067   \n",
              "2        2.332864     -2.908929      2.941076     -3.133755     7.399475   \n",
              "3        1.924908     -2.274820      1.640638     -2.479522     5.149405   \n",
              "4       -3.909992      4.973126     -4.583091      7.322591   -11.548117   \n",
              "..            ...           ...           ...           ...          ...   \n",
              "889      2.716950     -3.773576      3.432483     -3.184185     6.446299   \n",
              "890      2.423111     -2.884316      2.270691     -3.015852     6.600163   \n",
              "891     -2.720943      3.260452     -4.072395      3.835468    -7.015322   \n",
              "892      2.341196     -3.274098      3.072714     -3.649316     5.207879   \n",
              "893 -49879.184000  33768.227000 -14806.883000  11694.690000 -2608.811000   \n",
              "\n",
              "           1.1.1     0.1.1.1      1.1.1.1    0.1.1.1.1    1.1.1.1.1  ...  \\\n",
              "0      -3.390698    1.659329    -3.012199     3.436132    -2.051579  ...   \n",
              "1      -4.423811    1.772402    -4.427463     3.389840    -3.910992  ...   \n",
              "2      -3.701832    1.529097    -3.323052     2.685679    -2.423942  ...   \n",
              "3      -2.522085    2.035665    -4.024216     3.072954    -1.651766  ...   \n",
              "4       5.724316   -2.577478     6.573018    -5.460110     2.931455  ...   \n",
              "..           ...         ...          ...          ...          ...  ...   \n",
              "889    -5.202756    3.625214    -3.966662     3.847220    -3.245514  ...   \n",
              "890    -4.685534    2.986126    -4.408060     3.591517    -2.934106  ...   \n",
              "891     3.222818   -2.212169     4.496822    -3.261340     2.588654  ...   \n",
              "892    -2.868720    2.375783    -3.565799     2.690030    -2.718827  ...   \n",
              "893  2288.010500 -284.562840  1012.947500 -2789.188000  2361.170200  ...   \n",
              "\n",
              "          758       759       760       761       762       763       764  \\\n",
              "0    0.012692  0.059102  0.000541  0.006523 -0.018735  0.046029 -0.031323   \n",
              "1   -0.017646  0.061697  0.007149  0.000070  0.010949  0.033709 -0.033934   \n",
              "2    0.039987  0.010057 -0.038091 -0.033632 -0.018176 -0.017782  0.028502   \n",
              "3   -0.070302 -0.045334  0.016006  0.018444  0.056975  0.053900  0.043913   \n",
              "4    0.007845  0.069599 -0.017022  0.010712 -0.014984 -0.001413 -0.022211   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "889 -0.026921  0.033994  0.025864 -0.034472  0.024374 -0.022076 -0.001890   \n",
              "890  0.092023 -0.017697  0.014944 -0.065511  0.016395  0.007684 -0.016207   \n",
              "891 -0.050173  0.037019 -0.048023  0.000220 -0.033158  0.075531  0.064022   \n",
              "892 -0.008388  0.008983 -0.058522 -0.008533 -0.016264  0.003724 -0.005826   \n",
              "893  0.025953  0.049041 -0.015561 -0.064802  0.025398  0.007854 -0.010056   \n",
              "\n",
              "          765       766       767  \n",
              "0    0.021993 -0.059348  0.010890  \n",
              "1   -0.006214 -0.046594 -0.020140  \n",
              "2    0.009641 -0.035604 -0.009934  \n",
              "3    0.003843 -0.038083  0.000696  \n",
              "4   -0.025620  0.007522 -0.037539  \n",
              "..        ...       ...       ...  \n",
              "889  0.006269 -0.019845  0.016206  \n",
              "890 -0.008501  0.049251 -0.022232  \n",
              "891 -0.000590 -0.031904 -0.002240  \n",
              "892 -0.020846 -0.027097 -0.008377  \n",
              "893  0.026847 -0.055978 -0.010197  \n",
              "\n",
              "[894 rows x 786 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cffb88cb-6d7e-41ac-a089-31e38240453c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>0.1</th>\n",
              "      <th>1.1</th>\n",
              "      <th>0.1.1</th>\n",
              "      <th>1.1.1</th>\n",
              "      <th>0.1.1.1</th>\n",
              "      <th>1.1.1.1</th>\n",
              "      <th>0.1.1.1.1</th>\n",
              "      <th>1.1.1.1.1</th>\n",
              "      <th>...</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.349461</td>\n",
              "      <td>-2.602859</td>\n",
              "      <td>2.833252</td>\n",
              "      <td>-2.743249</td>\n",
              "      <td>6.893370</td>\n",
              "      <td>-3.390698</td>\n",
              "      <td>1.659329</td>\n",
              "      <td>-3.012199</td>\n",
              "      <td>3.436132</td>\n",
              "      <td>-2.051579</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012692</td>\n",
              "      <td>0.059102</td>\n",
              "      <td>0.000541</td>\n",
              "      <td>0.006523</td>\n",
              "      <td>-0.018735</td>\n",
              "      <td>0.046029</td>\n",
              "      <td>-0.031323</td>\n",
              "      <td>0.021993</td>\n",
              "      <td>-0.059348</td>\n",
              "      <td>0.010890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.731273</td>\n",
              "      <td>-4.053496</td>\n",
              "      <td>2.255937</td>\n",
              "      <td>-2.753675</td>\n",
              "      <td>7.221067</td>\n",
              "      <td>-4.423811</td>\n",
              "      <td>1.772402</td>\n",
              "      <td>-4.427463</td>\n",
              "      <td>3.389840</td>\n",
              "      <td>-3.910992</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.017646</td>\n",
              "      <td>0.061697</td>\n",
              "      <td>0.007149</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.010949</td>\n",
              "      <td>0.033709</td>\n",
              "      <td>-0.033934</td>\n",
              "      <td>-0.006214</td>\n",
              "      <td>-0.046594</td>\n",
              "      <td>-0.020140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.332864</td>\n",
              "      <td>-2.908929</td>\n",
              "      <td>2.941076</td>\n",
              "      <td>-3.133755</td>\n",
              "      <td>7.399475</td>\n",
              "      <td>-3.701832</td>\n",
              "      <td>1.529097</td>\n",
              "      <td>-3.323052</td>\n",
              "      <td>2.685679</td>\n",
              "      <td>-2.423942</td>\n",
              "      <td>...</td>\n",
              "      <td>0.039987</td>\n",
              "      <td>0.010057</td>\n",
              "      <td>-0.038091</td>\n",
              "      <td>-0.033632</td>\n",
              "      <td>-0.018176</td>\n",
              "      <td>-0.017782</td>\n",
              "      <td>0.028502</td>\n",
              "      <td>0.009641</td>\n",
              "      <td>-0.035604</td>\n",
              "      <td>-0.009934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.924908</td>\n",
              "      <td>-2.274820</td>\n",
              "      <td>1.640638</td>\n",
              "      <td>-2.479522</td>\n",
              "      <td>5.149405</td>\n",
              "      <td>-2.522085</td>\n",
              "      <td>2.035665</td>\n",
              "      <td>-4.024216</td>\n",
              "      <td>3.072954</td>\n",
              "      <td>-1.651766</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.070302</td>\n",
              "      <td>-0.045334</td>\n",
              "      <td>0.016006</td>\n",
              "      <td>0.018444</td>\n",
              "      <td>0.056975</td>\n",
              "      <td>0.053900</td>\n",
              "      <td>0.043913</td>\n",
              "      <td>0.003843</td>\n",
              "      <td>-0.038083</td>\n",
              "      <td>0.000696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-3.909992</td>\n",
              "      <td>4.973126</td>\n",
              "      <td>-4.583091</td>\n",
              "      <td>7.322591</td>\n",
              "      <td>-11.548117</td>\n",
              "      <td>5.724316</td>\n",
              "      <td>-2.577478</td>\n",
              "      <td>6.573018</td>\n",
              "      <td>-5.460110</td>\n",
              "      <td>2.931455</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007845</td>\n",
              "      <td>0.069599</td>\n",
              "      <td>-0.017022</td>\n",
              "      <td>0.010712</td>\n",
              "      <td>-0.014984</td>\n",
              "      <td>-0.001413</td>\n",
              "      <td>-0.022211</td>\n",
              "      <td>-0.025620</td>\n",
              "      <td>0.007522</td>\n",
              "      <td>-0.037539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>2.716950</td>\n",
              "      <td>-3.773576</td>\n",
              "      <td>3.432483</td>\n",
              "      <td>-3.184185</td>\n",
              "      <td>6.446299</td>\n",
              "      <td>-5.202756</td>\n",
              "      <td>3.625214</td>\n",
              "      <td>-3.966662</td>\n",
              "      <td>3.847220</td>\n",
              "      <td>-3.245514</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.026921</td>\n",
              "      <td>0.033994</td>\n",
              "      <td>0.025864</td>\n",
              "      <td>-0.034472</td>\n",
              "      <td>0.024374</td>\n",
              "      <td>-0.022076</td>\n",
              "      <td>-0.001890</td>\n",
              "      <td>0.006269</td>\n",
              "      <td>-0.019845</td>\n",
              "      <td>0.016206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>2.423111</td>\n",
              "      <td>-2.884316</td>\n",
              "      <td>2.270691</td>\n",
              "      <td>-3.015852</td>\n",
              "      <td>6.600163</td>\n",
              "      <td>-4.685534</td>\n",
              "      <td>2.986126</td>\n",
              "      <td>-4.408060</td>\n",
              "      <td>3.591517</td>\n",
              "      <td>-2.934106</td>\n",
              "      <td>...</td>\n",
              "      <td>0.092023</td>\n",
              "      <td>-0.017697</td>\n",
              "      <td>0.014944</td>\n",
              "      <td>-0.065511</td>\n",
              "      <td>0.016395</td>\n",
              "      <td>0.007684</td>\n",
              "      <td>-0.016207</td>\n",
              "      <td>-0.008501</td>\n",
              "      <td>0.049251</td>\n",
              "      <td>-0.022232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>891</th>\n",
              "      <td>-2.720943</td>\n",
              "      <td>3.260452</td>\n",
              "      <td>-4.072395</td>\n",
              "      <td>3.835468</td>\n",
              "      <td>-7.015322</td>\n",
              "      <td>3.222818</td>\n",
              "      <td>-2.212169</td>\n",
              "      <td>4.496822</td>\n",
              "      <td>-3.261340</td>\n",
              "      <td>2.588654</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.050173</td>\n",
              "      <td>0.037019</td>\n",
              "      <td>-0.048023</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>-0.033158</td>\n",
              "      <td>0.075531</td>\n",
              "      <td>0.064022</td>\n",
              "      <td>-0.000590</td>\n",
              "      <td>-0.031904</td>\n",
              "      <td>-0.002240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>892</th>\n",
              "      <td>2.341196</td>\n",
              "      <td>-3.274098</td>\n",
              "      <td>3.072714</td>\n",
              "      <td>-3.649316</td>\n",
              "      <td>5.207879</td>\n",
              "      <td>-2.868720</td>\n",
              "      <td>2.375783</td>\n",
              "      <td>-3.565799</td>\n",
              "      <td>2.690030</td>\n",
              "      <td>-2.718827</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008388</td>\n",
              "      <td>0.008983</td>\n",
              "      <td>-0.058522</td>\n",
              "      <td>-0.008533</td>\n",
              "      <td>-0.016264</td>\n",
              "      <td>0.003724</td>\n",
              "      <td>-0.005826</td>\n",
              "      <td>-0.020846</td>\n",
              "      <td>-0.027097</td>\n",
              "      <td>-0.008377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893</th>\n",
              "      <td>-49879.184000</td>\n",
              "      <td>33768.227000</td>\n",
              "      <td>-14806.883000</td>\n",
              "      <td>11694.690000</td>\n",
              "      <td>-2608.811000</td>\n",
              "      <td>2288.010500</td>\n",
              "      <td>-284.562840</td>\n",
              "      <td>1012.947500</td>\n",
              "      <td>-2789.188000</td>\n",
              "      <td>2361.170200</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025953</td>\n",
              "      <td>0.049041</td>\n",
              "      <td>-0.015561</td>\n",
              "      <td>-0.064802</td>\n",
              "      <td>0.025398</td>\n",
              "      <td>0.007854</td>\n",
              "      <td>-0.010056</td>\n",
              "      <td>0.026847</td>\n",
              "      <td>-0.055978</td>\n",
              "      <td>-0.010197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>894 rows × 786 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cffb88cb-6d7e-41ac-a089-31e38240453c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cffb88cb-6d7e-41ac-a089-31e38240453c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cffb88cb-6d7e-41ac-a089-31e38240453c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 338
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = pd.DataFrame(logits.detach().numpy())"
      ],
      "metadata": {
        "id": "kPUollGIGOZ1"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.concat([my_tensor,my_data],axis=1)"
      ],
      "metadata": {
        "id": "xB1C9nIbGus4"
      },
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "wH_ulxSqHQCh",
        "outputId": "30a135d4-9a5f-45d8-e2c4-835c41f5e256"
      },
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                0             1             0             1           0.1  \\\n",
              "0        2.712114     -3.334514      1.349461     -2.602859      2.833252   \n",
              "1        2.403871     -3.430604      3.731273     -4.053496      2.255937   \n",
              "2        3.137182     -2.637305      2.332864     -2.908929      2.941076   \n",
              "3        2.768392     -1.913109      1.924908     -2.274820      1.640638   \n",
              "4       -6.659483      3.588523     -3.909992      4.973126     -4.583091   \n",
              "..            ...           ...           ...           ...           ...   \n",
              "889      2.762979     -3.326401      2.716950     -3.773576      3.432483   \n",
              "890      2.342873     -2.420721      2.423111     -2.884316      2.270691   \n",
              "891     -4.647464      2.340831     -2.720943      3.260452     -4.072395   \n",
              "892      2.879259     -3.046558      2.341196     -3.274098      3.072714   \n",
              "893 -68321.148438  73912.156250 -49879.184000  33768.227000 -14806.883000   \n",
              "\n",
              "              1.1        0.1.1        1.1.1     0.1.1.1      1.1.1.1  ...  \\\n",
              "0       -2.743249     6.893370    -3.390698    1.659329    -3.012199  ...   \n",
              "1       -2.753675     7.221067    -4.423811    1.772402    -4.427463  ...   \n",
              "2       -3.133755     7.399475    -3.701832    1.529097    -3.323052  ...   \n",
              "3       -2.479522     5.149405    -2.522085    2.035665    -4.024216  ...   \n",
              "4        7.322591   -11.548117     5.724316   -2.577478     6.573018  ...   \n",
              "..            ...          ...          ...         ...          ...  ...   \n",
              "889     -3.184185     6.446299    -5.202756    3.625214    -3.966662  ...   \n",
              "890     -3.015852     6.600163    -4.685534    2.986126    -4.408060  ...   \n",
              "891      3.835468    -7.015322     3.222818   -2.212169     4.496822  ...   \n",
              "892     -3.649316     5.207879    -2.868720    2.375783    -3.565799  ...   \n",
              "893  11694.690000 -2608.811000  2288.010500 -284.562840  1012.947500  ...   \n",
              "\n",
              "          758       759       760       761       762       763       764  \\\n",
              "0    0.012692  0.059102  0.000541  0.006523 -0.018735  0.046029 -0.031323   \n",
              "1   -0.017646  0.061697  0.007149  0.000070  0.010949  0.033709 -0.033934   \n",
              "2    0.039987  0.010057 -0.038091 -0.033632 -0.018176 -0.017782  0.028502   \n",
              "3   -0.070302 -0.045334  0.016006  0.018444  0.056975  0.053900  0.043913   \n",
              "4    0.007845  0.069599 -0.017022  0.010712 -0.014984 -0.001413 -0.022211   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "889 -0.026921  0.033994  0.025864 -0.034472  0.024374 -0.022076 -0.001890   \n",
              "890  0.092023 -0.017697  0.014944 -0.065511  0.016395  0.007684 -0.016207   \n",
              "891 -0.050173  0.037019 -0.048023  0.000220 -0.033158  0.075531  0.064022   \n",
              "892 -0.008388  0.008983 -0.058522 -0.008533 -0.016264  0.003724 -0.005826   \n",
              "893  0.025953  0.049041 -0.015561 -0.064802  0.025398  0.007854 -0.010056   \n",
              "\n",
              "          765       766       767  \n",
              "0    0.021993 -0.059348  0.010890  \n",
              "1   -0.006214 -0.046594 -0.020140  \n",
              "2    0.009641 -0.035604 -0.009934  \n",
              "3    0.003843 -0.038083  0.000696  \n",
              "4   -0.025620  0.007522 -0.037539  \n",
              "..        ...       ...       ...  \n",
              "889  0.006269 -0.019845  0.016206  \n",
              "890 -0.008501  0.049251 -0.022232  \n",
              "891 -0.000590 -0.031904 -0.002240  \n",
              "892 -0.020846 -0.027097 -0.008377  \n",
              "893  0.026847 -0.055978 -0.010197  \n",
              "\n",
              "[894 rows x 788 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c403baef-3112-41fc-be8b-264fd512125c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>0.1</th>\n",
              "      <th>1.1</th>\n",
              "      <th>0.1.1</th>\n",
              "      <th>1.1.1</th>\n",
              "      <th>0.1.1.1</th>\n",
              "      <th>1.1.1.1</th>\n",
              "      <th>...</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.712114</td>\n",
              "      <td>-3.334514</td>\n",
              "      <td>1.349461</td>\n",
              "      <td>-2.602859</td>\n",
              "      <td>2.833252</td>\n",
              "      <td>-2.743249</td>\n",
              "      <td>6.893370</td>\n",
              "      <td>-3.390698</td>\n",
              "      <td>1.659329</td>\n",
              "      <td>-3.012199</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012692</td>\n",
              "      <td>0.059102</td>\n",
              "      <td>0.000541</td>\n",
              "      <td>0.006523</td>\n",
              "      <td>-0.018735</td>\n",
              "      <td>0.046029</td>\n",
              "      <td>-0.031323</td>\n",
              "      <td>0.021993</td>\n",
              "      <td>-0.059348</td>\n",
              "      <td>0.010890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.403871</td>\n",
              "      <td>-3.430604</td>\n",
              "      <td>3.731273</td>\n",
              "      <td>-4.053496</td>\n",
              "      <td>2.255937</td>\n",
              "      <td>-2.753675</td>\n",
              "      <td>7.221067</td>\n",
              "      <td>-4.423811</td>\n",
              "      <td>1.772402</td>\n",
              "      <td>-4.427463</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.017646</td>\n",
              "      <td>0.061697</td>\n",
              "      <td>0.007149</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.010949</td>\n",
              "      <td>0.033709</td>\n",
              "      <td>-0.033934</td>\n",
              "      <td>-0.006214</td>\n",
              "      <td>-0.046594</td>\n",
              "      <td>-0.020140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.137182</td>\n",
              "      <td>-2.637305</td>\n",
              "      <td>2.332864</td>\n",
              "      <td>-2.908929</td>\n",
              "      <td>2.941076</td>\n",
              "      <td>-3.133755</td>\n",
              "      <td>7.399475</td>\n",
              "      <td>-3.701832</td>\n",
              "      <td>1.529097</td>\n",
              "      <td>-3.323052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.039987</td>\n",
              "      <td>0.010057</td>\n",
              "      <td>-0.038091</td>\n",
              "      <td>-0.033632</td>\n",
              "      <td>-0.018176</td>\n",
              "      <td>-0.017782</td>\n",
              "      <td>0.028502</td>\n",
              "      <td>0.009641</td>\n",
              "      <td>-0.035604</td>\n",
              "      <td>-0.009934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.768392</td>\n",
              "      <td>-1.913109</td>\n",
              "      <td>1.924908</td>\n",
              "      <td>-2.274820</td>\n",
              "      <td>1.640638</td>\n",
              "      <td>-2.479522</td>\n",
              "      <td>5.149405</td>\n",
              "      <td>-2.522085</td>\n",
              "      <td>2.035665</td>\n",
              "      <td>-4.024216</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.070302</td>\n",
              "      <td>-0.045334</td>\n",
              "      <td>0.016006</td>\n",
              "      <td>0.018444</td>\n",
              "      <td>0.056975</td>\n",
              "      <td>0.053900</td>\n",
              "      <td>0.043913</td>\n",
              "      <td>0.003843</td>\n",
              "      <td>-0.038083</td>\n",
              "      <td>0.000696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-6.659483</td>\n",
              "      <td>3.588523</td>\n",
              "      <td>-3.909992</td>\n",
              "      <td>4.973126</td>\n",
              "      <td>-4.583091</td>\n",
              "      <td>7.322591</td>\n",
              "      <td>-11.548117</td>\n",
              "      <td>5.724316</td>\n",
              "      <td>-2.577478</td>\n",
              "      <td>6.573018</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007845</td>\n",
              "      <td>0.069599</td>\n",
              "      <td>-0.017022</td>\n",
              "      <td>0.010712</td>\n",
              "      <td>-0.014984</td>\n",
              "      <td>-0.001413</td>\n",
              "      <td>-0.022211</td>\n",
              "      <td>-0.025620</td>\n",
              "      <td>0.007522</td>\n",
              "      <td>-0.037539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>2.762979</td>\n",
              "      <td>-3.326401</td>\n",
              "      <td>2.716950</td>\n",
              "      <td>-3.773576</td>\n",
              "      <td>3.432483</td>\n",
              "      <td>-3.184185</td>\n",
              "      <td>6.446299</td>\n",
              "      <td>-5.202756</td>\n",
              "      <td>3.625214</td>\n",
              "      <td>-3.966662</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.026921</td>\n",
              "      <td>0.033994</td>\n",
              "      <td>0.025864</td>\n",
              "      <td>-0.034472</td>\n",
              "      <td>0.024374</td>\n",
              "      <td>-0.022076</td>\n",
              "      <td>-0.001890</td>\n",
              "      <td>0.006269</td>\n",
              "      <td>-0.019845</td>\n",
              "      <td>0.016206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>2.342873</td>\n",
              "      <td>-2.420721</td>\n",
              "      <td>2.423111</td>\n",
              "      <td>-2.884316</td>\n",
              "      <td>2.270691</td>\n",
              "      <td>-3.015852</td>\n",
              "      <td>6.600163</td>\n",
              "      <td>-4.685534</td>\n",
              "      <td>2.986126</td>\n",
              "      <td>-4.408060</td>\n",
              "      <td>...</td>\n",
              "      <td>0.092023</td>\n",
              "      <td>-0.017697</td>\n",
              "      <td>0.014944</td>\n",
              "      <td>-0.065511</td>\n",
              "      <td>0.016395</td>\n",
              "      <td>0.007684</td>\n",
              "      <td>-0.016207</td>\n",
              "      <td>-0.008501</td>\n",
              "      <td>0.049251</td>\n",
              "      <td>-0.022232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>891</th>\n",
              "      <td>-4.647464</td>\n",
              "      <td>2.340831</td>\n",
              "      <td>-2.720943</td>\n",
              "      <td>3.260452</td>\n",
              "      <td>-4.072395</td>\n",
              "      <td>3.835468</td>\n",
              "      <td>-7.015322</td>\n",
              "      <td>3.222818</td>\n",
              "      <td>-2.212169</td>\n",
              "      <td>4.496822</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.050173</td>\n",
              "      <td>0.037019</td>\n",
              "      <td>-0.048023</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>-0.033158</td>\n",
              "      <td>0.075531</td>\n",
              "      <td>0.064022</td>\n",
              "      <td>-0.000590</td>\n",
              "      <td>-0.031904</td>\n",
              "      <td>-0.002240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>892</th>\n",
              "      <td>2.879259</td>\n",
              "      <td>-3.046558</td>\n",
              "      <td>2.341196</td>\n",
              "      <td>-3.274098</td>\n",
              "      <td>3.072714</td>\n",
              "      <td>-3.649316</td>\n",
              "      <td>5.207879</td>\n",
              "      <td>-2.868720</td>\n",
              "      <td>2.375783</td>\n",
              "      <td>-3.565799</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008388</td>\n",
              "      <td>0.008983</td>\n",
              "      <td>-0.058522</td>\n",
              "      <td>-0.008533</td>\n",
              "      <td>-0.016264</td>\n",
              "      <td>0.003724</td>\n",
              "      <td>-0.005826</td>\n",
              "      <td>-0.020846</td>\n",
              "      <td>-0.027097</td>\n",
              "      <td>-0.008377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893</th>\n",
              "      <td>-68321.148438</td>\n",
              "      <td>73912.156250</td>\n",
              "      <td>-49879.184000</td>\n",
              "      <td>33768.227000</td>\n",
              "      <td>-14806.883000</td>\n",
              "      <td>11694.690000</td>\n",
              "      <td>-2608.811000</td>\n",
              "      <td>2288.010500</td>\n",
              "      <td>-284.562840</td>\n",
              "      <td>1012.947500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025953</td>\n",
              "      <td>0.049041</td>\n",
              "      <td>-0.015561</td>\n",
              "      <td>-0.064802</td>\n",
              "      <td>0.025398</td>\n",
              "      <td>0.007854</td>\n",
              "      <td>-0.010056</td>\n",
              "      <td>0.026847</td>\n",
              "      <td>-0.055978</td>\n",
              "      <td>-0.010197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>894 rows × 788 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c403baef-3112-41fc-be8b-264fd512125c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c403baef-3112-41fc-be8b-264fd512125c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c403baef-3112-41fc-be8b-264fd512125c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 341
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.to_csv('updatedEmbs.csv')"
      ],
      "metadata": {
        "id": "-eN1WO9hHRGq"
      },
      "execution_count": 342,
      "outputs": []
    }
  ]
}