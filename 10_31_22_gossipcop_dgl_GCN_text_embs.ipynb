{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88709491857c44d480ded0a2a3520ac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4157ad21f6dd435e93e40819feaf4ba7",
              "IPY_MODEL_432d7ec3416c4a3cb8b201a21ee504e3"
            ],
            "layout": "IPY_MODEL_91a9b7f321624e5dbcd356be3671b5cc"
          }
        },
        "4157ad21f6dd435e93e40819feaf4ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f6c073cab7b4ded812e5ba49921d0fa",
            "placeholder": "​",
            "style": "IPY_MODEL_edbd366f2a744b44bb79292f9ec2d39d",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "432d7ec3416c4a3cb8b201a21ee504e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fc52151549c4fb5b27f36b06094fbb8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78f3a5b3317c4e7aa86c4ad0429eb5f4",
            "value": 0.8624472573839662
          }
        },
        "91a9b7f321624e5dbcd356be3671b5cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f6c073cab7b4ded812e5ba49921d0fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edbd366f2a744b44bb79292f9ec2d39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fc52151549c4fb5b27f36b06094fbb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78f3a5b3317c4e7aa86c4ad0429eb5f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/verma-saloni/Thesis-Work/blob/main/10_31_22_gossipcop_dgl_GCN_text_embs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xpTeMbYZwmf7"
      },
      "outputs": [],
      "source": [
        "!pip -qq install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxNEaltli8H_",
        "outputId": "8d40f64f-5b35-4756-db22-4f08533f1b19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "base_dir = Path(\"/gdrive/MyDrive/ResearchFND\")\n",
        "assert base_dir.exists()"
      ],
      "metadata": {
        "id": "E1KzQ-APvvbj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "ywmJcojaOk4E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I-Bd_WNiLkOc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import json"
      ],
      "metadata": {
        "id": "Pi19HZwbOnpK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_id = 'gossipcop'\n",
        "text_embeddings = 'sbert' # options: sbert, ft"
      ],
      "metadata": {
        "id": "5XqiYfvoPECo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(base_dir/f'{dataset_id}_agg.csv')\n",
        "df.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "767e14d9-ffab-4c55-e1f9-71eb00b3133e",
        "id": "9fSNOSLsOnpP"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title text tweets  \\\n",
              "0      Kendall   Kylie Jenner Jenner NOT Upset Up...  NaN     []   \n",
              "1      Kim Kardashian Dethroned Dethroned By Khlo...  NaN     []   \n",
              "\n",
              "                                            retweets label  url  num_retweets  \\\n",
              "0  ['995423424741888001', '995461685166202880', '...  fake  NaN             3   \n",
              "1  ['848843565027516416', '849030801970868224', '...  fake  NaN             3   \n",
              "\n",
              "   log_num_retweets  num_tweets  log_num_tweets  \n",
              "0          1.386294           0             0.0  \n",
              "1          1.386294           0             0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2e263be-9113-4756-9b32-b683628f0380\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>tweets</th>\n",
              "      <th>retweets</th>\n",
              "      <th>label</th>\n",
              "      <th>url</th>\n",
              "      <th>num_retweets</th>\n",
              "      <th>log_num_retweets</th>\n",
              "      <th>num_tweets</th>\n",
              "      <th>log_num_tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kendall   Kylie Jenner Jenner NOT Upset Up...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>['995423424741888001', '995461685166202880', '...</td>\n",
              "      <td>fake</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>1.386294</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kim Kardashian Dethroned Dethroned By Khlo...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>['848843565027516416', '849030801970868224', '...</td>\n",
              "      <td>fake</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>1.386294</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2e263be-9113-4756-9b32-b683628f0380')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e2e263be-9113-4756-9b32-b683628f0380 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e2e263be-9113-4756-9b32-b683628f0380');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweets'] = df.tweets.map(ast.literal_eval)"
      ],
      "metadata": {
        "id": "qjvjCmHpOnpS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_tweeted = df.tweets.map(lambda x: [int(e['user_id']) for e in x])"
      ],
      "metadata": {
        "id": "CovrRHkeOnpU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(users_tweeted), sum(users_tweeted.map(len) > 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74412c1-289c-40fe-abed-77ec5b4d4554",
        "id": "VmjV_67MOnpW"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19968, 2117)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GNN"
      ],
      "metadata": {
        "id": "gI6AtSffmcqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "FfaOg08xmfEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "!pip install dgl wandb"
      ],
      "metadata": {
        "id": "xjD3rF-MmwKa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e393e55-1a03-40c2-8e3d-2315e3ce6638"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dgl\n",
            "  Downloading dgl-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 4.8 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.13.4-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 59.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.6)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl) (4.64.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.7.3)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n",
            "\u001b[K     |████████████████████████████████| 291 kB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2022.9.24)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 68.9 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 69.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 74.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 52.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 59.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 68.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 60.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 46.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 58.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 47.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 74.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 58.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 61.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 61.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=8f574a8a7a8383096a4eb090d41c1bc8d5fc40c7409a0506d9883cd2b54a58b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, psutil, pathtools, GitPython, docker-pycreds, wandb, dgl\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed GitPython-3.1.29 dgl-0.9.1 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 psutil-5.9.3 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env DGLBACKEND=pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w96ReVNpwHTY",
        "outputId": "aaca4a73-251e-47f7-ed0f-36aa74907d12"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: DGLBACKEND=pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "import torch\n",
        "import dgl\n",
        "\n",
        "import wandb\n",
        "import IPython.display as ipd"
      ],
      "metadata": {
        "id": "yp52MLR1mdxA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "u2i = {}\n",
        "\n",
        "follow_src = []\n",
        "follow_dst = []\n",
        "with jsonlines.open(base_dir/\"followers.jsonl\") as reader:\n",
        "    for line in reader:\n",
        "        v = line[\"user_id\"]\n",
        "        if v not in u2i:\n",
        "            u2i[v] = len(u2i)\n",
        "        for u in line[\"followers\"]:\n",
        "            if u not in u2i:\n",
        "                u2i[u] = len(u2i)\n",
        "            follow_src.append(u2i[u])\n",
        "            follow_dst.append(u2i[v])"
      ],
      "metadata": {
        "id": "h3XOCaEwoMvx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open(base_dir/\"following.jsonl\") as reader:\n",
        "    for line in reader:\n",
        "        u = line[\"user_id\"]\n",
        "        if u not in u2i:\n",
        "            u2i[u] = len(u2i)\n",
        "        for v in line[\"following\"]:\n",
        "            if v not in u2i:\n",
        "                u2i[v] = len(u2i)\n",
        "            follow_src.append(u2i[u])\n",
        "            follow_dst.append(u2i[v])"
      ],
      "metadata": {
        "id": "sWJFoJ_4oMvz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_src = []\n",
        "tweet_dst = []\n",
        "\n",
        "for v, l in users_tweeted.iteritems():\n",
        "    if not len(l):\n",
        "        continue\n",
        "    for u in l:\n",
        "        u = int(u)\n",
        "        if u in u2i:\n",
        "            tweet_src.append(u2i[u])\n",
        "            tweet_dst.append(v)"
      ],
      "metadata": {
        "id": "DWM9MEXDpiyX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embs = np.load(base_dir/f'{dataset_id}_{text_embeddings}_fulltext_embeddings.npy')\n",
        "text_embs.shape"
      ],
      "metadata": {
        "id": "CxkWUv799WnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7fcaf92-6f13-407c-e65e-ac07c21396f1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19968, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_users = len(set(follow_src+follow_dst+tweet_src))"
      ],
      "metadata": {
        "id": "CWYAnXYALbwo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "follow_src = torch.tensor(follow_src)\n",
        "follow_dst = torch.tensor(follow_dst)\n",
        "tweet_src = torch.tensor(tweet_src)\n",
        "tweet_dst = torch.tensor(tweet_dst)\n",
        "\n",
        "graph = dgl.heterograph({\n",
        "    ('user', 'follow', 'user'): (follow_src, follow_dst),\n",
        "    ('user', 'followed-by', 'user'): (follow_dst, follow_src),\n",
        "    ('user', 'tweet', 'article'): (tweet_src, tweet_dst),\n",
        "    ('article', 'tweeted-by', 'user'): (tweet_dst, tweet_src)},\n",
        "    {'article':len(df), 'user':num_users}\n",
        ")\n",
        "\n",
        "graph.nodes['user'].data['feat'] = torch.arange(graph.num_nodes('user'))\n",
        "graph.nodes['article'].data['feat'] = torch.tensor(text_embs)\n",
        "graph.nodes['article'].data['label'] = torch.tensor((df.label==\"real\").to_numpy()).long()"
      ],
      "metadata": {
        "id": "0o-_TVpQ2Ag8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_gsQ0ELzwpN",
        "outputId": "0c33bdec-2e7d-436e-d112-1c46a98c463f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Graph(num_nodes={'article': 19968, 'user': 31792},\n",
              "      num_edges={('article', 'tweeted-by', 'user'): 588, ('user', 'follow', 'user'): 48409, ('user', 'followed-by', 'user'): 48409, ('user', 'tweet', 'article'): 588},\n",
              "      metagraph=[('article', 'user', 'tweeted-by'), ('user', 'user', 'follow'), ('user', 'user', 'followed-by'), ('user', 'article', 'tweet')])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(shuffle=True, random_state=124)"
      ],
      "metadata": {
        "id": "_wR6tgjekCGZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = graph.ndata['label']['article']\n",
        "\n",
        "train_idx, valid_idx = next(skf.split(labels, labels))"
      ],
      "metadata": {
        "id": "vUYRWGCqkNJw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "bz8FvZRmkrFt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = 3\n",
        "n_neighbors = 10"
      ],
      "metadata": {
        "id": "wNxxuCrpVGUN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = dgl.dataloading.NeighborSampler([n_neighbors]*n_layers)\n",
        "train_loader = dgl.dataloading.DataLoader(\n",
        "    graph,\n",
        "    {'article':train_idx},\n",
        "    sampler,\n",
        "    device=device,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "9lWNOaTwj86U"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_sampler = dgl.dataloading.NeighborSampler([-1]*n_layers)\n",
        "eval_loader = dgl.dataloading.DataLoader(\n",
        "    graph,\n",
        "    {'article':valid_idx},\n",
        "    eval_sampler,\n",
        "    device=device,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "2P7O9wNi1GXS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKlDjmxnkvaT",
        "outputId": "92c9d502-ae07-4a14-8e71-6ed33e4d90e6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/dgl/dataloading/dataloader.py:859: DGLWarning: Dataloader CPU affinity opt is not enabled, consider switching it on (see enable_cpu_affinity() or CPU best practices for DGL [https://docs.dgl.ai/tutorials/cpu/cpu_best_practises.html])\n",
            "  dgl_warning(f'Dataloader CPU affinity opt is not enabled, consider switching it on '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "a_NnqbhIvgQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "d_emb_dict = defaultdict(lambda: 64)\n",
        "\n",
        "def flatten_dict(d):\n",
        "    for k, v in d.items():\n",
        "        d[k] = v.flatten(1)\n",
        "    return d"
      ],
      "metadata": {
        "id": "oSGL7euSjzR-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NodeEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Node embedding layer. Handles input node features of varying dimentionality.\n",
        "    \n",
        "    Parameters:\n",
        "    - n_nodes: dict[str,int] - a dictionary containing number of nodes per type\n",
        "    for id only nodes (without precmputed features)\n",
        "    - d_in: dict[str,int] - a dictionary mapping node type to the node feature dim\n",
        "    for nodes with precomputed vector features\n",
        "    - d_emb: int - size of output feature vector for all node types\n",
        "    - proj_nodes: list[str] - optional list of node types with vector features to be\n",
        "    processed with linear projection. If None keys of `d_in` are used\n",
        "    - embed_nodes: list[str] - optional list of node types without vector features to be\n",
        "    processed with embedding layer. If None keys of `n_nnodes` are used\n",
        "\n",
        "    Inputs:\n",
        "    - nx - dict[str,Tensor] - dictionary containing input node features per node type\n",
        "\n",
        "    Outputs:\n",
        "    - out - dict[str, Tensor] - dictionary of node embedding tensors of shape [bs, d_emb]\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes:dict, d_in:dict, d_emb:int, proj_nodes:list=None, embed_nodes:list=None):\n",
        "        super().__init__()\n",
        "        self.proj_nodes = proj_nodes if proj_nodes is not None else list(d_in.keys())\n",
        "        self.embed_nodes = embed_nodes if embed_nodes is not None else list(n_nodes.keys())\n",
        "        self.emb = nn.ModuleDict({k:nn.Embedding(n_nodes[k], d_emb) for k in self.embed_nodes})\n",
        "        self.proj = nn.ModuleDict({k:nn.Linear(d_in[k], d_emb, bias=False) for k in self.proj_nodes})\n",
        "        self.init()\n",
        "\n",
        "    def forward(self, nx):\n",
        "        out = {}\n",
        "        for k, m  in self.emb.items():\n",
        "            out[k] = m(nx[k])\n",
        "        for k, m  in self.proj.items():\n",
        "            out[k] = m(nx[k])\n",
        "        return out\n",
        "\n",
        "    def init(self):\n",
        "        for _, m in self.emb.items():\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "        for _, m in self.proj.items():\n",
        "            torch.nn.init.xavier_uniform_(m.weight)"
      ],
      "metadata": {
        "id": "YGqNa5UVgfio"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual connection. Computes output node features as:\n",
        "    x_out_dst = GraphConv(graph, x_in_all) + x_in_dst\n",
        "    \"\"\"\n",
        "    def __init__(self, conv):\n",
        "        super().__init__()\n",
        "        self.conv = conv\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        h = self.conv(graph, x)\n",
        "        res = x[1]\n",
        "        return h + res"
      ],
      "metadata": {
        "id": "vN3hV1uAv1L2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_proj = NodeEmbedding({k:graph.num_nodes(k) for k in [\"user\"]}, {\"article\":text_embs.shape[1]}, 64)\n",
        "conv = dgl.nn.HeteroGraphConv({rel:Residual(dgl.nn.GraphConv(64, 64, allow_zero_in_degree=True)) for rel in graph.etypes})"
      ],
      "metadata": {
        "id": "yc-vcrjpw-6T"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blocks = batch[-1]\n",
        "block = blocks[0]\n",
        "x = block.ndata['feat']\n",
        "\n",
        "with torch.no_grad():\n",
        "    h = in_proj(x)\n",
        "    res = conv(block, h)"
      ],
      "metadata": {
        "id": "uKQd2Wn3xDSn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res['article'].shape, res['user'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEYSQp_oyewZ",
        "outputId": "67437967-cd47-481f-e5b0-86c56323b110"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 64]), torch.Size([0, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder for heterogenous graph using GCN layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_in, d_h, etypes, n_layers=2, dropout=0.0):\n",
        "        super().__init__()\n",
        "        ds = [d_in] + [d_h] * n_layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            dgl.nn.HeteroGraphConv({\n",
        "                rel : Residual(dgl.nn.GraphConv(ds[i], ds[i+1], allow_zero_in_degree=True)) for rel in etypes\n",
        "            }) for i in range(n_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, blocks, x):\n",
        "        \n",
        "        for layer, block in zip(self.layers, blocks):\n",
        "            x = layer(block, x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "bceu2k_-OgIG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph.ndata['feat']['article'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tfx9M1N37yc",
        "outputId": "ee8a39de-05ee-4687-d5e5-e4ce5678dbb0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([19968, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, g, d_h:int, n_layers:int, tgt_ntype:str, emb_nodes:list=['user'], proj_nodes:list=['article']):\n",
        "        super().__init__()\n",
        "        self.tgt_ntype = tgt_ntype\n",
        "        self.in_proj = NodeEmbedding(\n",
        "            {k:g.num_nodes(k) for k in emb_nodes}, \n",
        "            {k:graph.ndata['feat'][k].shape[1] for k in proj_nodes},\n",
        "            d_h\n",
        "        )\n",
        "        self.encoder = Encoder(d_h, d_h, g.etypes)\n",
        "        self.head = nn.Linear(d_h, 2)\n",
        "\n",
        "    def forward(self, blocks, x):\n",
        "        h = self.in_proj(x)\n",
        "        h = self.encoder(blocks, h)\n",
        "        return self.head(h[self.tgt_ntype])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_embeddings(self, graph, x):\n",
        "        h = self.emb(x)\n",
        "        h = self.encoder(graph, h)\n",
        "        return h[self.tgt_ntype]"
      ],
      "metadata": {
        "id": "hFfx0YmfXoom"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GNN(graph, 128, n_layers, 'article')\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(blocks, x)"
      ],
      "metadata": {
        "id": "wFkuDQQgd3-v"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhjU0-N1zShT",
        "outputId": "92c13a2d-5326-481c-d8aa-f5e43f2b86ce"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(logits, labels):\n",
        "    return (logits.argmax(-1) == labels).float().mean()"
      ],
      "metadata": {
        "id": "BunQmwFY8Xso"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(shuffle=True, random_state=124)"
      ],
      "metadata": {
        "id": "faEjToOWbAQl"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "metrics = [accuracy_score, f1_score, precision_score, recall_score]\n",
        "def get_name(score_func):\n",
        "    return score_func.__name__.split(\"_\")[0]"
      ],
      "metadata": {
        "id": "1vjIXm2wM6_T"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter:\n",
        "\n",
        "    def __init__(self, store_vals=False, store_avgs=False):\n",
        "        self.store_vals = store_vals\n",
        "        self.store_avgs = store_avgs\n",
        "        if store_vals: self.values = []\n",
        "        if store_avgs: self.avgs = []\n",
        "        self.tot, self.n = 0, 0\n",
        "\n",
        "    def update(self, v, n=1):\n",
        "        if self.store_vals: self.values.append(v)\n",
        "        self.n += n\n",
        "        self.tot += v*n\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        if self.n == 0:\n",
        "            return\n",
        "        return self.tot / self.n\n",
        "\n",
        "    def reset(self):\n",
        "        if self.store_avgs and self.avg: self.avgs.append(self.avg)\n",
        "        self.tot, self.n = 0, 0"
      ],
      "metadata": {
        "id": "S40fYMCU7nzh"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = torch.tensor([1/(labels==0).sum(), 1/(labels==1).sum()])*len(labels)"
      ],
      "metadata": {
        "id": "tCjiRKIjXdd2"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(fold, train_idx, valid_idx, params):\n",
        "\n",
        "    model = GNN(graph, 128, n_layers, 'article')\n",
        "    opt = torch.optim.Adam(model.parameters(), params['lr'])\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        opt, factor=params[\"lr_sched_factor\"], patience=params[\"patience\"], verbose=True\n",
        "    )\n",
        "\n",
        "    train_loss = AverageMeter(store_avgs=True)\n",
        "    train_acc = AverageMeter(store_avgs=True)\n",
        "    valid_loss = AverageMeter(store_avgs=True)\n",
        "    valid_acc = AverageMeter(store_avgs=True)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(params['n_epochs']):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            blocks = batch[-1]\n",
        "            x = blocks[0].ndata['feat']\n",
        "            logits = model(blocks, x)\n",
        "            \n",
        "            labels = blocks[-1].dstdata['label']['article']\n",
        "            loss = F.cross_entropy(logits, labels, weight=class_weights)\n",
        "            acc = accuracy(logits, labels)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            train_loss.update(loss.item(), len(labels))\n",
        "            train_acc.update(acc, len(labels))\n",
        "\n",
        "        model.eval()\n",
        "        for i, batch in enumerate(eval_loader):\n",
        "            blocks = batch[-1]\n",
        "            x = blocks[0].ndata['feat']\n",
        "            with torch.no_grad():\n",
        "                logits = model(blocks, x)\n",
        "\n",
        "                labels = blocks[-1].dstdata['label']['article']\n",
        "                val_loss = F.cross_entropy(logits, labels, weight=class_weights)\n",
        "                val_acc = accuracy(logits, labels)\n",
        "\n",
        "                valid_loss.update(val_loss.item(), len(labels))\n",
        "                valid_acc.update(val_acc, len(labels))\n",
        "        \n",
        "        scheduler.step(valid_loss.avg)\n",
        "        wandb.log({'train_loss':loss.item(), 'train_acc':acc, 'valid_loss':val_loss.item(), 'valid_acc':val_acc}, step=epoch)\n",
        "        print(f\"{epoch+1:>3}: Train loss {train_loss.avg:.4f}, acc {train_acc.avg:.4f}%; validation loss {valid_loss.avg:.4f}, acc {valid_acc.avg:.4f}%\")\n",
        "        \n",
        "        if valid_acc.avg >= best_acc:\n",
        "            best_acc = valid_acc.avg\n",
        "            torch.save(model.state_dict(), f'models/model-{fold}.pt')\n",
        "        \n",
        "        train_loss.reset()\n",
        "        train_acc.reset()\n",
        "        valid_loss.reset()\n",
        "        valid_acc.reset()\n",
        "\n",
        "        \n",
        "\n",
        "    # load best model and evaluate\n",
        "    model.load_state_dict(torch.load(f'models/model-{fold}.pt'))\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targs = []\n",
        "    for i, batch in enumerate(eval_loader):\n",
        "        blocks = batch[-1]\n",
        "        x = blocks[0].ndata['feat']\n",
        "        with torch.no_grad():\n",
        "            logits = model(blocks, x)\n",
        "\n",
        "            labels = blocks[-1].dstdata['label']['article']\n",
        "            \n",
        "        preds.append(logits.argmax(-1).cpu().numpy())\n",
        "        targs.append(labels.cpu().numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    targs = np.concatenate(targs)\n",
        "    eval_results = {get_name(f):f(y_pred=preds, y_true=targs) for f in metrics}\n",
        "    print(\"Final evaluation results:\")\n",
        "    for k,v in eval_results.items():\n",
        "        print(f\"{k:<16}{v:.4f}\")\n",
        "    \n",
        "    wandb.log(eval_results)\n",
        "    wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
        "                            y_true=targs, preds=preds,\n",
        "                            class_names=[\"Fake\", \"Real\"])})\n",
        "\n",
        "    return {\n",
        "        'train_loss':train_loss,\n",
        "        'train_acc':train_acc,\n",
        "        'valid_loss':valid_loss,\n",
        "        'valid_acc':valid_acc\n",
        "    }"
      ],
      "metadata": {
        "id": "4lvLwrItbAQp"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"n_epochs\":400,\n",
        "    'bs': 16,\n",
        "    'lr':4e-3,\n",
        "    \"seed\":124,\n",
        "    \"lr_sched_factor\":0.5,\n",
        "    \"patience\":10\n",
        "}\n",
        "\n",
        "labels = graph.ndata['label']['article']"
      ],
      "metadata": {
        "id": "VWKvv1xebAQu"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('models'):\n",
        "    os.mkdir('models')\n",
        "\n",
        "GROUP = \"sbert-fulltext-gcn-updatedE400\"\n",
        "for fold_id, (train_idx, valid_idx) in enumerate(skf.split(labels, labels)):\n",
        "    ipd.clear_output()\n",
        "    with wandb.init(entity=\"saloniteam\", project=\"nofolds\", group=GROUP, name=f\"{GROUP}-fold-{fold_id}\") as run:\n",
        "        log = train(fold_id, train_idx, valid_idx, params)\n",
        "    break"
      ],
      "metadata": {
        "id": "7qBu9i1ebAQx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "88709491857c44d480ded0a2a3520ac7",
            "4157ad21f6dd435e93e40819feaf4ba7",
            "432d7ec3416c4a3cb8b201a21ee504e3",
            "91a9b7f321624e5dbcd356be3671b5cc",
            "4f6c073cab7b4ded812e5ba49921d0fa",
            "edbd366f2a744b44bb79292f9ec2d39d",
            "1fc52151549c4fb5b27f36b06094fbb8",
            "78f3a5b3317c4e7aa86c4ad0429eb5f4"
          ]
        },
        "outputId": "9e50bfb1-31dc-4fa2-c23c-dac838c53492"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221031_094705-1ymlymzz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/saloniteam/nofolds/runs/1ymlymzz\" target=\"_blank\">sbert-fulltext-gcn-updatedE400-fold-0</a></strong> to <a href=\"https://wandb.ai/saloniteam/nofolds\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/dgl/dataloading/dataloader.py:859: DGLWarning: Dataloader CPU affinity opt is not enabled, consider switching it on (see enable_cpu_affinity() or CPU best practices for DGL [https://docs.dgl.ai/tutorials/cpu/cpu_best_practises.html])\n",
            "  dgl_warning(f'Dataloader CPU affinity opt is not enabled, consider switching it on '\n",
            "/usr/local/lib/python3.7/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1: Train loss 0.5011, acc 0.7669%; validation loss 0.4955, acc 0.7734%\n",
            "  2: Train loss 0.4751, acc 0.7957%; validation loss 0.4842, acc 0.7589%\n",
            "  3: Train loss 0.4487, acc 0.8011%; validation loss 0.4782, acc 0.7772%\n",
            "  4: Train loss 0.4399, acc 0.8061%; validation loss 0.4937, acc 0.7749%\n",
            "  5: Train loss 0.4396, acc 0.8070%; validation loss 0.4820, acc 0.8152%\n",
            "  6: Train loss 0.4349, acc 0.8101%; validation loss 0.6291, acc 0.7932%\n",
            "  7: Train loss 0.4342, acc 0.8098%; validation loss 0.4915, acc 0.8090%\n",
            "  8: Train loss 0.4282, acc 0.8105%; validation loss 0.5306, acc 0.7957%\n",
            "  9: Train loss 0.4615, acc 0.8063%; validation loss 0.5889, acc 0.8287%\n",
            " 10: Train loss 0.4629, acc 0.8098%; validation loss 1.5307, acc 0.8110%\n",
            " 11: Train loss 0.4361, acc 0.8138%; validation loss 0.5584, acc 0.8025%\n",
            " 12: Train loss 0.4239, acc 0.8173%; validation loss 0.5151, acc 0.7666%\n",
            " 13: Train loss 0.4215, acc 0.8170%; validation loss 0.5628, acc 0.7942%\n",
            "Epoch 00014: reducing learning rate of group 0 to 2.0000e-03.\n",
            " 14: Train loss 0.4308, acc 0.8166%; validation loss 0.5207, acc 0.8187%\n",
            " 15: Train loss 0.4185, acc 0.8217%; validation loss 0.5506, acc 0.8100%\n",
            " 16: Train loss 0.4168, acc 0.8217%; validation loss 0.5130, acc 0.8090%\n",
            " 17: Train loss 0.4088, acc 0.8221%; validation loss 0.5217, acc 0.7661%\n",
            " 18: Train loss 0.4111, acc 0.8216%; validation loss 0.5333, acc 0.7967%\n",
            " 19: Train loss 0.4154, acc 0.8211%; validation loss 0.5210, acc 0.7952%\n",
            " 20: Train loss 0.4129, acc 0.8232%; validation loss 0.5409, acc 0.7959%\n",
            " 21: Train loss 0.4094, acc 0.8236%; validation loss 0.5183, acc 0.8010%\n",
            " 22: Train loss 0.4052, acc 0.8228%; validation loss 0.5822, acc 0.7977%\n",
            " 23: Train loss 0.4089, acc 0.8237%; validation loss 0.5684, acc 0.8097%\n",
            " 24: Train loss 0.4069, acc 0.8212%; validation loss 0.5455, acc 0.8127%\n",
            "Epoch 00025: reducing learning rate of group 0 to 1.0000e-03.\n",
            " 25: Train loss 0.4089, acc 0.8240%; validation loss 0.6022, acc 0.7879%\n",
            " 26: Train loss 0.4014, acc 0.8254%; validation loss 0.5441, acc 0.8022%\n",
            " 27: Train loss 0.4007, acc 0.8268%; validation loss 0.5594, acc 0.8030%\n",
            " 28: Train loss 0.3996, acc 0.8265%; validation loss 0.5414, acc 0.8147%\n",
            " 29: Train loss 0.3975, acc 0.8259%; validation loss 0.5669, acc 0.7907%\n",
            " 30: Train loss 0.3985, acc 0.8255%; validation loss 0.5758, acc 0.8085%\n",
            " 31: Train loss 0.3976, acc 0.8226%; validation loss 0.5627, acc 0.8077%\n",
            " 32: Train loss 0.3977, acc 0.8266%; validation loss 0.5742, acc 0.8050%\n",
            " 33: Train loss 0.3966, acc 0.8255%; validation loss 0.5611, acc 0.7972%\n",
            " 34: Train loss 0.3962, acc 0.8257%; validation loss 0.5410, acc 0.8087%\n",
            " 35: Train loss 0.3966, acc 0.8238%; validation loss 0.5775, acc 0.8155%\n",
            "Epoch 00036: reducing learning rate of group 0 to 5.0000e-04.\n",
            " 36: Train loss 0.3950, acc 0.8277%; validation loss 0.5631, acc 0.8157%\n",
            " 37: Train loss 0.3926, acc 0.8271%; validation loss 0.5685, acc 0.8080%\n",
            " 38: Train loss 0.3916, acc 0.8270%; validation loss 0.5732, acc 0.7959%\n",
            " 39: Train loss 0.3920, acc 0.8271%; validation loss 0.5813, acc 0.8077%\n",
            " 40: Train loss 0.3921, acc 0.8270%; validation loss 0.5804, acc 0.8042%\n",
            " 41: Train loss 0.3912, acc 0.8260%; validation loss 0.5766, acc 0.8022%\n",
            " 42: Train loss 0.3915, acc 0.8270%; validation loss 0.5696, acc 0.8017%\n",
            " 43: Train loss 0.3914, acc 0.8266%; validation loss 0.5738, acc 0.8032%\n",
            " 44: Train loss 0.3907, acc 0.8279%; validation loss 0.5933, acc 0.8052%\n",
            " 45: Train loss 0.3901, acc 0.8281%; validation loss 0.5998, acc 0.7952%\n",
            " 46: Train loss 0.3905, acc 0.8268%; validation loss 0.5715, acc 0.8142%\n",
            "Epoch 00047: reducing learning rate of group 0 to 2.5000e-04.\n",
            " 47: Train loss 0.3901, acc 0.8279%; validation loss 0.5917, acc 0.8087%\n",
            " 48: Train loss 0.3884, acc 0.8272%; validation loss 0.5941, acc 0.8100%\n",
            " 49: Train loss 0.3881, acc 0.8283%; validation loss 0.5994, acc 0.8107%\n",
            " 50: Train loss 0.3887, acc 0.8288%; validation loss 0.6041, acc 0.8120%\n",
            " 51: Train loss 0.3896, acc 0.8297%; validation loss 0.5800, acc 0.8017%\n",
            " 52: Train loss 0.3885, acc 0.8263%; validation loss 0.5903, acc 0.8112%\n",
            " 53: Train loss 0.3889, acc 0.8285%; validation loss 0.5880, acc 0.8017%\n",
            " 54: Train loss 0.3890, acc 0.8289%; validation loss 0.5902, acc 0.7984%\n",
            " 55: Train loss 0.3884, acc 0.8281%; validation loss 0.5996, acc 0.8045%\n",
            " 56: Train loss 0.3879, acc 0.8284%; validation loss 0.5934, acc 0.8045%\n",
            " 57: Train loss 0.3880, acc 0.8276%; validation loss 0.5884, acc 0.8107%\n",
            "Epoch 00058: reducing learning rate of group 0 to 1.2500e-04.\n",
            " 58: Train loss 0.3872, acc 0.8294%; validation loss 0.5961, acc 0.8017%\n",
            " 59: Train loss 0.3865, acc 0.8280%; validation loss 0.5973, acc 0.8037%\n",
            " 60: Train loss 0.3871, acc 0.8287%; validation loss 0.5917, acc 0.8045%\n",
            " 61: Train loss 0.3887, acc 0.8292%; validation loss 0.5932, acc 0.8060%\n",
            " 62: Train loss 0.3860, acc 0.8287%; validation loss 0.5941, acc 0.7999%\n",
            " 63: Train loss 0.3864, acc 0.8283%; validation loss 0.5975, acc 0.8047%\n",
            " 64: Train loss 0.3873, acc 0.8290%; validation loss 0.5992, acc 0.8062%\n",
            " 65: Train loss 0.3865, acc 0.8293%; validation loss 0.5994, acc 0.8050%\n",
            " 66: Train loss 0.3872, acc 0.8300%; validation loss 0.5992, acc 0.8027%\n",
            " 67: Train loss 0.3863, acc 0.8275%; validation loss 0.6028, acc 0.8065%\n",
            " 68: Train loss 0.3863, acc 0.8308%; validation loss 0.6023, acc 0.8005%\n",
            "Epoch 00069: reducing learning rate of group 0 to 6.2500e-05.\n",
            " 69: Train loss 0.3860, acc 0.8285%; validation loss 0.6001, acc 0.8032%\n",
            " 70: Train loss 0.3863, acc 0.8282%; validation loss 0.6020, acc 0.8042%\n",
            " 71: Train loss 0.3862, acc 0.8294%; validation loss 0.6030, acc 0.8042%\n",
            " 72: Train loss 0.3860, acc 0.8298%; validation loss 0.6012, acc 0.8045%\n",
            " 73: Train loss 0.3847, acc 0.8290%; validation loss 0.6000, acc 0.8052%\n",
            " 74: Train loss 0.3854, acc 0.8294%; validation loss 0.6002, acc 0.8050%\n",
            " 75: Train loss 0.3859, acc 0.8295%; validation loss 0.6002, acc 0.8040%\n",
            " 76: Train loss 0.3858, acc 0.8288%; validation loss 0.5994, acc 0.8045%\n",
            " 77: Train loss 0.3850, acc 0.8310%; validation loss 0.5990, acc 0.8027%\n",
            " 78: Train loss 0.3862, acc 0.8292%; validation loss 0.5990, acc 0.8045%\n",
            " 79: Train loss 0.3865, acc 0.8287%; validation loss 0.5978, acc 0.8047%\n",
            "Epoch 00080: reducing learning rate of group 0 to 3.1250e-05.\n",
            " 80: Train loss 0.3857, acc 0.8292%; validation loss 0.5961, acc 0.8040%\n",
            " 81: Train loss 0.3857, acc 0.8295%; validation loss 0.5958, acc 0.8040%\n",
            " 82: Train loss 0.3855, acc 0.8280%; validation loss 0.5959, acc 0.8050%\n",
            " 83: Train loss 0.3849, acc 0.8290%; validation loss 0.5967, acc 0.8055%\n",
            " 84: Train loss 0.3853, acc 0.8296%; validation loss 0.5967, acc 0.8052%\n",
            " 85: Train loss 0.3850, acc 0.8295%; validation loss 0.5974, acc 0.8055%\n",
            " 86: Train loss 0.3852, acc 0.8306%; validation loss 0.5959, acc 0.8050%\n",
            " 87: Train loss 0.3858, acc 0.8295%; validation loss 0.5966, acc 0.8042%\n",
            " 88: Train loss 0.3855, acc 0.8283%; validation loss 0.5961, acc 0.8057%\n",
            " 89: Train loss 0.3855, acc 0.8300%; validation loss 0.5950, acc 0.8042%\n",
            " 90: Train loss 0.3858, acc 0.8290%; validation loss 0.5959, acc 0.8057%\n",
            "Epoch 00091: reducing learning rate of group 0 to 1.5625e-05.\n",
            " 91: Train loss 0.3845, acc 0.8304%; validation loss 0.5962, acc 0.8060%\n",
            " 92: Train loss 0.3858, acc 0.8291%; validation loss 0.5959, acc 0.8055%\n",
            " 93: Train loss 0.3851, acc 0.8300%; validation loss 0.5957, acc 0.8050%\n",
            " 94: Train loss 0.3853, acc 0.8295%; validation loss 0.5956, acc 0.8050%\n",
            " 95: Train loss 0.3859, acc 0.8290%; validation loss 0.5951, acc 0.8052%\n",
            " 96: Train loss 0.3845, acc 0.8304%; validation loss 0.5959, acc 0.8052%\n",
            " 97: Train loss 0.3850, acc 0.8297%; validation loss 0.5955, acc 0.8052%\n",
            " 98: Train loss 0.3850, acc 0.8295%; validation loss 0.5955, acc 0.8052%\n",
            " 99: Train loss 0.3852, acc 0.8292%; validation loss 0.5960, acc 0.8052%\n",
            "100: Train loss 0.3850, acc 0.8300%; validation loss 0.5964, acc 0.8045%\n",
            "101: Train loss 0.3848, acc 0.8288%; validation loss 0.5966, acc 0.8047%\n",
            "Epoch 00102: reducing learning rate of group 0 to 7.8125e-06.\n",
            "102: Train loss 0.3853, acc 0.8289%; validation loss 0.5967, acc 0.8052%\n",
            "103: Train loss 0.3852, acc 0.8296%; validation loss 0.5968, acc 0.8052%\n",
            "104: Train loss 0.3854, acc 0.8293%; validation loss 0.5963, acc 0.8052%\n",
            "105: Train loss 0.3848, acc 0.8297%; validation loss 0.5965, acc 0.8052%\n",
            "106: Train loss 0.3848, acc 0.8292%; validation loss 0.5968, acc 0.8057%\n",
            "107: Train loss 0.3848, acc 0.8295%; validation loss 0.5970, acc 0.8052%\n",
            "108: Train loss 0.3852, acc 0.8297%; validation loss 0.5970, acc 0.8052%\n",
            "109: Train loss 0.3858, acc 0.8297%; validation loss 0.5969, acc 0.8052%\n",
            "110: Train loss 0.3849, acc 0.8292%; validation loss 0.5967, acc 0.8052%\n",
            "111: Train loss 0.3854, acc 0.8298%; validation loss 0.5969, acc 0.8050%\n",
            "112: Train loss 0.3850, acc 0.8290%; validation loss 0.5973, acc 0.8052%\n",
            "Epoch 00113: reducing learning rate of group 0 to 3.9063e-06.\n",
            "113: Train loss 0.3852, acc 0.8297%; validation loss 0.5973, acc 0.8052%\n",
            "114: Train loss 0.3855, acc 0.8291%; validation loss 0.5972, acc 0.8047%\n",
            "115: Train loss 0.3850, acc 0.8293%; validation loss 0.5972, acc 0.8052%\n",
            "116: Train loss 0.3856, acc 0.8294%; validation loss 0.5972, acc 0.8052%\n",
            "117: Train loss 0.3842, acc 0.8296%; validation loss 0.5972, acc 0.8052%\n",
            "118: Train loss 0.3849, acc 0.8294%; validation loss 0.5973, acc 0.8052%\n",
            "119: Train loss 0.3844, acc 0.8295%; validation loss 0.5974, acc 0.8052%\n",
            "120: Train loss 0.3845, acc 0.8293%; validation loss 0.5973, acc 0.8052%\n",
            "121: Train loss 0.3859, acc 0.8294%; validation loss 0.5975, acc 0.8055%\n",
            "122: Train loss 0.3848, acc 0.8295%; validation loss 0.5975, acc 0.8055%\n",
            "123: Train loss 0.3849, acc 0.8292%; validation loss 0.5973, acc 0.8052%\n",
            "Epoch 00124: reducing learning rate of group 0 to 1.9531e-06.\n",
            "124: Train loss 0.3850, acc 0.8295%; validation loss 0.5972, acc 0.8052%\n",
            "125: Train loss 0.3843, acc 0.8294%; validation loss 0.5972, acc 0.8055%\n",
            "126: Train loss 0.3849, acc 0.8294%; validation loss 0.5972, acc 0.8055%\n",
            "127: Train loss 0.3857, acc 0.8296%; validation loss 0.5972, acc 0.8055%\n",
            "128: Train loss 0.3846, acc 0.8296%; validation loss 0.5972, acc 0.8055%\n",
            "129: Train loss 0.3853, acc 0.8294%; validation loss 0.5972, acc 0.8055%\n",
            "130: Train loss 0.3847, acc 0.8294%; validation loss 0.5973, acc 0.8055%\n",
            "131: Train loss 0.3849, acc 0.8295%; validation loss 0.5973, acc 0.8055%\n",
            "132: Train loss 0.3852, acc 0.8295%; validation loss 0.5973, acc 0.8055%\n",
            "133: Train loss 0.3844, acc 0.8295%; validation loss 0.5973, acc 0.8055%\n",
            "134: Train loss 0.3846, acc 0.8295%; validation loss 0.5972, acc 0.8055%\n",
            "Epoch 00135: reducing learning rate of group 0 to 9.7656e-07.\n",
            "135: Train loss 0.3855, acc 0.8294%; validation loss 0.5974, acc 0.8055%\n",
            "136: Train loss 0.3850, acc 0.8293%; validation loss 0.5974, acc 0.8055%\n",
            "137: Train loss 0.3850, acc 0.8294%; validation loss 0.5974, acc 0.8055%\n",
            "138: Train loss 0.3852, acc 0.8294%; validation loss 0.5974, acc 0.8055%\n",
            "139: Train loss 0.3850, acc 0.8295%; validation loss 0.5974, acc 0.8055%\n",
            "140: Train loss 0.3844, acc 0.8295%; validation loss 0.5974, acc 0.8055%\n",
            "141: Train loss 0.3849, acc 0.8293%; validation loss 0.5974, acc 0.8055%\n",
            "142: Train loss 0.3842, acc 0.8294%; validation loss 0.5975, acc 0.8055%\n",
            "143: Train loss 0.3851, acc 0.8294%; validation loss 0.5975, acc 0.8055%\n",
            "144: Train loss 0.3851, acc 0.8293%; validation loss 0.5975, acc 0.8055%\n",
            "145: Train loss 0.3851, acc 0.8293%; validation loss 0.5975, acc 0.8055%\n",
            "Epoch 00146: reducing learning rate of group 0 to 4.8828e-07.\n",
            "146: Train loss 0.3848, acc 0.8295%; validation loss 0.5975, acc 0.8055%\n",
            "147: Train loss 0.3850, acc 0.8295%; validation loss 0.5975, acc 0.8055%\n",
            "148: Train loss 0.3847, acc 0.8294%; validation loss 0.5975, acc 0.8055%\n",
            "149: Train loss 0.3839, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "150: Train loss 0.3855, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "151: Train loss 0.3844, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "152: Train loss 0.3848, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "153: Train loss 0.3851, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "154: Train loss 0.3857, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "155: Train loss 0.3846, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "156: Train loss 0.3849, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "Epoch 00157: reducing learning rate of group 0 to 2.4414e-07.\n",
            "157: Train loss 0.3848, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "158: Train loss 0.3848, acc 0.8294%; validation loss 0.5975, acc 0.8055%\n",
            "159: Train loss 0.3848, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "160: Train loss 0.3852, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "161: Train loss 0.3852, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "162: Train loss 0.3841, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "163: Train loss 0.3849, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "164: Train loss 0.3846, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "165: Train loss 0.3854, acc 0.8293%; validation loss 0.5975, acc 0.8055%\n",
            "166: Train loss 0.3846, acc 0.8293%; validation loss 0.5975, acc 0.8055%\n",
            "167: Train loss 0.3851, acc 0.8294%; validation loss 0.5975, acc 0.8055%\n",
            "Epoch 00168: reducing learning rate of group 0 to 1.2207e-07.\n",
            "168: Train loss 0.3840, acc 0.8294%; validation loss 0.5975, acc 0.8055%\n",
            "169: Train loss 0.3853, acc 0.8295%; validation loss 0.5975, acc 0.8055%\n",
            "170: Train loss 0.3849, acc 0.8293%; validation loss 0.5975, acc 0.8055%\n",
            "171: Train loss 0.3854, acc 0.8294%; validation loss 0.5975, acc 0.8055%\n",
            "172: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "173: Train loss 0.3852, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "174: Train loss 0.3853, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "175: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "176: Train loss 0.3850, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "177: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "178: Train loss 0.3855, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "Epoch 00179: reducing learning rate of group 0 to 6.1035e-08.\n",
            "179: Train loss 0.3852, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "180: Train loss 0.3858, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "181: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "182: Train loss 0.3848, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "183: Train loss 0.3848, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "184: Train loss 0.3855, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "185: Train loss 0.3850, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "186: Train loss 0.3847, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "187: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "188: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "189: Train loss 0.3845, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "Epoch 00190: reducing learning rate of group 0 to 3.0518e-08.\n",
            "190: Train loss 0.3854, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "191: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "192: Train loss 0.3852, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "193: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "194: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "195: Train loss 0.3848, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "196: Train loss 0.3837, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "197: Train loss 0.3849, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "198: Train loss 0.3851, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "199: Train loss 0.3851, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "200: Train loss 0.3849, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "Epoch 00201: reducing learning rate of group 0 to 1.5259e-08.\n",
            "201: Train loss 0.3836, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "202: Train loss 0.3852, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "203: Train loss 0.3855, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "204: Train loss 0.3849, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "205: Train loss 0.3846, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "206: Train loss 0.3856, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "207: Train loss 0.3846, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "208: Train loss 0.3855, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "209: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "210: Train loss 0.3849, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "211: Train loss 0.3855, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "212: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "213: Train loss 0.3842, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "214: Train loss 0.3846, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "215: Train loss 0.3843, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "216: Train loss 0.3845, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "217: Train loss 0.3854, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "218: Train loss 0.3859, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "219: Train loss 0.3857, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "220: Train loss 0.3844, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "221: Train loss 0.3843, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "222: Train loss 0.3851, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "223: Train loss 0.3849, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "224: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "225: Train loss 0.3847, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "226: Train loss 0.3854, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "227: Train loss 0.3851, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "228: Train loss 0.3854, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "229: Train loss 0.3858, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "230: Train loss 0.3849, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "231: Train loss 0.3848, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "232: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "233: Train loss 0.3853, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "234: Train loss 0.3852, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "235: Train loss 0.3856, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "236: Train loss 0.3854, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "237: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "238: Train loss 0.3853, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "239: Train loss 0.3849, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "240: Train loss 0.3846, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "241: Train loss 0.3841, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "242: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "243: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "244: Train loss 0.3857, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "245: Train loss 0.3852, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "246: Train loss 0.3854, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "247: Train loss 0.3843, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "248: Train loss 0.3848, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "249: Train loss 0.3852, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "250: Train loss 0.3850, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "251: Train loss 0.3851, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "252: Train loss 0.3843, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "253: Train loss 0.3849, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "254: Train loss 0.3847, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "255: Train loss 0.3846, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "256: Train loss 0.3851, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "257: Train loss 0.3842, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "258: Train loss 0.3846, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "259: Train loss 0.3846, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "260: Train loss 0.3837, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "261: Train loss 0.3853, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "262: Train loss 0.3843, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "263: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "264: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "265: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "266: Train loss 0.3852, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "267: Train loss 0.3844, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "268: Train loss 0.3848, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "269: Train loss 0.3859, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "270: Train loss 0.3851, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "271: Train loss 0.3848, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "272: Train loss 0.3851, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "273: Train loss 0.3855, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "274: Train loss 0.3853, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "275: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "276: Train loss 0.3851, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "277: Train loss 0.3854, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "278: Train loss 0.3856, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "279: Train loss 0.3851, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "280: Train loss 0.3842, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "281: Train loss 0.3838, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "282: Train loss 0.3848, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "283: Train loss 0.3844, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "284: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "285: Train loss 0.3847, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "286: Train loss 0.3847, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "287: Train loss 0.3844, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "288: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "289: Train loss 0.3844, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "290: Train loss 0.3851, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "291: Train loss 0.3851, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "292: Train loss 0.3851, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "293: Train loss 0.3848, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "294: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "295: Train loss 0.3851, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "296: Train loss 0.3838, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "297: Train loss 0.3846, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "298: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "299: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "300: Train loss 0.3856, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "301: Train loss 0.3853, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "302: Train loss 0.3849, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "303: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "304: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "305: Train loss 0.3853, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "306: Train loss 0.3843, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "307: Train loss 0.3847, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "308: Train loss 0.3844, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "309: Train loss 0.3851, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "310: Train loss 0.3853, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "311: Train loss 0.3848, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "312: Train loss 0.3853, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "313: Train loss 0.3852, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "314: Train loss 0.3851, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "315: Train loss 0.3849, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "316: Train loss 0.3847, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "317: Train loss 0.3849, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "318: Train loss 0.3848, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "319: Train loss 0.3848, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "320: Train loss 0.3846, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "321: Train loss 0.3847, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "322: Train loss 0.3850, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "323: Train loss 0.3846, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "324: Train loss 0.3845, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "325: Train loss 0.3854, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "326: Train loss 0.3846, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "327: Train loss 0.3849, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "328: Train loss 0.3857, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "329: Train loss 0.3849, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "330: Train loss 0.3848, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "331: Train loss 0.3845, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "332: Train loss 0.3853, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "333: Train loss 0.3849, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "334: Train loss 0.3844, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "335: Train loss 0.3846, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "336: Train loss 0.3856, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "337: Train loss 0.3850, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "338: Train loss 0.3861, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "339: Train loss 0.3838, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "340: Train loss 0.3855, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "341: Train loss 0.3848, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "342: Train loss 0.3852, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "343: Train loss 0.3857, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "344: Train loss 0.3853, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "345: Train loss 0.3846, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "346: Train loss 0.3850, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "347: Train loss 0.3842, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "348: Train loss 0.3850, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "349: Train loss 0.3851, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "350: Train loss 0.3847, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "351: Train loss 0.3853, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "352: Train loss 0.3849, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "353: Train loss 0.3843, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "354: Train loss 0.3846, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "355: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "356: Train loss 0.3844, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "357: Train loss 0.3846, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "358: Train loss 0.3846, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "359: Train loss 0.3856, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "360: Train loss 0.3844, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "361: Train loss 0.3844, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "362: Train loss 0.3854, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "363: Train loss 0.3858, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "364: Train loss 0.3847, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "365: Train loss 0.3845, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "366: Train loss 0.3848, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "367: Train loss 0.3848, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "368: Train loss 0.3849, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "369: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "370: Train loss 0.3851, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "371: Train loss 0.3853, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "372: Train loss 0.3851, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "373: Train loss 0.3857, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "374: Train loss 0.3843, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "375: Train loss 0.3858, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "376: Train loss 0.3854, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "377: Train loss 0.3846, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "378: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "379: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "380: Train loss 0.3843, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "381: Train loss 0.3849, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "382: Train loss 0.3861, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "383: Train loss 0.3855, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "384: Train loss 0.3855, acc 0.8293%; validation loss 0.5976, acc 0.8055%\n",
            "385: Train loss 0.3851, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "386: Train loss 0.3855, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "387: Train loss 0.3851, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "388: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "389: Train loss 0.3837, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "390: Train loss 0.3852, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "391: Train loss 0.3846, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "392: Train loss 0.3850, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "393: Train loss 0.3850, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "394: Train loss 0.3856, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "395: Train loss 0.3841, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "396: Train loss 0.3849, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "397: Train loss 0.3847, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "398: Train loss 0.3848, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "399: Train loss 0.3838, acc 0.8295%; validation loss 0.5976, acc 0.8055%\n",
            "400: Train loss 0.3855, acc 0.8294%; validation loss 0.5976, acc 0.8055%\n",
            "Final evaluation results:\n",
            "accuracy        0.8287\n",
            "f1              0.8875\n",
            "precision       0.9011\n",
            "recall          0.8743\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.862447…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88709491857c44d480ded0a2a3520ac7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>f1</td><td>▁</td></tr><tr><td>precision</td><td>▁</td></tr><tr><td>recall</td><td>▁</td></tr><tr><td>train_acc</td><td>▃▄▅▆▅▅▅▄▆▄▅▂▅▅▃▅▅▄▄▁█▂█▇▅▃▄▂▅▅▇▅▄▅▂▅▃▃▅▅</td></tr><tr><td>train_loss</td><td>▅▅▇▄▃▃▂▄▂▄▂▆▅▃▅▃▄▂▂▇▃▂▁▁▃█▃▅▄▇▂▃▄▅▅▇▄▄▃▆</td></tr><tr><td>valid_acc</td><td>█████▁██████████████████████████████████</td></tr><tr><td>valid_loss</td><td>▁▅▄██▇██████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.82874</td></tr><tr><td>f1</td><td>0.8875</td></tr><tr><td>precision</td><td>0.90114</td></tr><tr><td>recall</td><td>0.87427</td></tr><tr><td>train_acc</td><td>0.84211</td></tr><tr><td>train_loss</td><td>0.46313</td></tr><tr><td>valid_acc</td><td>0.92308</td></tr><tr><td>valid_loss</td><td>2.63928</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">sbert-fulltext-gcn-updatedE400-fold-0</strong>: <a href=\"https://wandb.ai/saloniteam/nofolds/runs/1ymlymzz\" target=\"_blank\">https://wandb.ai/saloniteam/nofolds/runs/1ymlymzz</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221031_094705-1ymlymzz/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GNN(graph, 128, n_layers, \"article\")\n",
        "\n",
        "model.load_state_dict(torch.load(f'models/model-{fold_id}.pt'))"
      ],
      "metadata": {
        "id": "wj6KA7GN5Q8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "779e3370-5970-4e79-9ea1-d3d12da21888"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}